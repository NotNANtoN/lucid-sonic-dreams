{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# params that need to be set for each computing node\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "gpu_avail = 12\n",
    "base_folder = \"/raid/8wiehe/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-plate",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "PYTHONPATH = sys.executable\n",
    "\n",
    "# set song\n",
    "song_name = \"songs/Daft_Punk_Harder_Better_Faster.mp3\"\n",
    "#song_name = \"songs/vbnd_daughter_of_the_sun.mp3\"\n",
    "#song_name = \"free_songs/tannenbaum_by_dan_lerch.mp3\"\n",
    "#song_name = \"songs/ratatat_Loud_Pipes.mp3\"\n",
    "#song_name = \"songs/gnossi_1.mp3\"\n",
    "#song_name = \"songs/henne_song.mp3\"\n",
    "#song_name = \"songs/Präludium_1_bach.mp3\"\n",
    "#song_name = \"songs/Präludium_2_Bach.mp3\"\n",
    "#song_name = \"songs/Blumen.wav\"\n",
    "#song_name = \"songs/Misty.mp3\"\n",
    "#song_name = \"songs/space_is_the_place_men_I_trust.mp3\"\n",
    "#song_name = \"songs/opus_men_I_trust.mp3\"\n",
    "#song_name = \"songs/any_colour_you_like_pink_floyd_hd_studio_quality_7032261705832661515.mp3\"\n",
    "\n",
    "# song params\n",
    "offset = 0\n",
    "duration = None\n",
    "\n",
    "# set some parameters based on how much memory is available\n",
    "sideX, sideY = 656, 368\n",
    "clip_batch_size = 4\n",
    "gpt_name = \"gpt2\"\n",
    "if gpu_avail >= 8:\n",
    "    clip_batch_size = 4\n",
    "    gpt_name = \"neo1.3\"\n",
    "elif gpu_avail >= 12:\n",
    "    clip_batch_size = 4\n",
    "    gpt_name = \"neo2.7\"\n",
    "    sideX, sideY = 880, 484\n",
    "# general params\n",
    "fps = 30\n",
    "boost_fps = 60\n",
    "# substep params\n",
    "sub_steps = 12  # constant substeps if variable substeps are not used\n",
    "use_variable_substeps = True\n",
    "min_substeps = 3\n",
    "max_substeps = 15\n",
    "# musicnn params\n",
    "input_overlap = 1 / 30\n",
    "input_length = 3\n",
    "# post processing params\n",
    "upscale = False\n",
    "twice_upscale = False\n",
    "# video params\n",
    "total_effect_strength = 0.2\n",
    "ema_val = 0.2 # 0.99 is too strong\n",
    "ema_val_latent = 0.3\n",
    "\n",
    "lpips_weight = 2.2  # 1.5 is good for 300 sub_steps but more could work too.\n",
    "# prompts\n",
    "base_img_path = \"../CLIP_playground/base_images/\"\n",
    "# settings for prompts\n",
    "k = 5\n",
    "prompt_mode = \"weighted_top_k\" # top_k, weighted_top_k, gpt\n",
    "use_mean_dirs = True\n",
    "mood_weight = 0.33  # how much to weigh the current story vs the predicted \"feelings\" - should be between 0.1-0.4, otherwise the story disappears\n",
    "\n",
    "prefix = \"\"  # prefix for ALL prompts\n",
    "taggram_mode = \"feelings\" # full, feelings\n",
    "general_theme = \"\" #\". In the style of James Gurney.\"\n",
    "create_gpt_artstyle = True\n",
    "num_themes = 10 # num GPT themes to sample\n",
    "create_clusters = True\n",
    "gpt_cluster_prefix = \"\"  # prefix for clusters\n",
    "do_create_gpt_cluster_stories = True\n",
    "use_k_means = False\n",
    "min_clusters = 3\n",
    "max_clusters = 13\n",
    "cluster_time_weight = 0.1  # how much weight to give to the time component while clustering in proportion to the maximum feature\n",
    "ema_val_clustering = 0.96\n",
    "\n",
    "gpt_story_top_k = 2  # number of k top cluster stories that will be consideren in CLIP guidance\n",
    "img_theme = None # base_img_path + \"hot-dog.jpg\"\n",
    "# base_img_path + \"Autumn_1875_Frederic_Edwin_Church.jpg\"\n",
    "#\". By Pete Mohrbacher.\"\n",
    "#\". In the style of 'The Persistence of Memory' by Dali.\" #\" by madziowa_p.\" #\" by Jan Brueghel the Elder.\"  #\" by Salvador Dali.\" #\" by Greg Rutkowski.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-marshall",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "resampled_path = \"tmp/resampled.wav\"\n",
    "os.makedirs(\"tmp\", exist_ok=True)\n",
    "\n",
    "# load song and resample to 16k Hz\n",
    "sr = 16000\n",
    "raw_song, old_sr = librosa.load(song_name, offset=offset, duration=duration)\n",
    "song = librosa.resample(raw_song, old_sr, sr)\n",
    "soundfile.write(resampled_path, song, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mustovi_utils import get_taggram\n",
    "    \n",
    "tag_dfs_folder = \"./tmp/tag_dfs\"\n",
    "os.makedirs(tag_dfs_folder, exist_ok=True)\n",
    "key_song_name = song_name.split(\"/\")[-1].split(\".\")[0]\n",
    "tag_df_name = f\"{key_song_name}_{input_length}_{int(1 / input_overlap)}_{offset}_{duration}.csv\"\n",
    "tag_df_path = os.path.join(tag_dfs_folder, tag_df_name)\n",
    "if os.path.exists(tag_df_path):\n",
    "    normed_tag_df = pd.read_csv(tag_df_path, index_col=0)\n",
    "else:\n",
    "    normed_tag_df = get_taggram(resampled_path, input_overlap, input_length)\n",
    "    normed_tag_df.to_csv(tag_df_path)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# ordered by importance\n",
    "show_all_labels = False\n",
    "if show_all_labels:\n",
    "    plt.figure(figsize=(12, 20))\n",
    "    show_df = normed_tag_df.T.copy()\n",
    "    show_df[\"mean\"] = normed_tag_df.mean(axis=0)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = [\"violin\", \"strings\", \"sitar\", \"piano\", \"harpsichord\", \n",
    "               \"harp\", \"guitar\", \"drums\", \"flute\", \"synth\", \"cello\"]\n",
    "genres = [\"techno\", \"soul\", \"rock\", \"rnb\", \"punk\", \"pop\", \"opera\", \n",
    "          \"oldies\", \"new age\", \"metal\", \"jazz\", \"indie rock\", \"indie pop\",\n",
    "          \"indie\", \"indian\", \"heavy metal\", \"hard rock\", \"funk\", \"folk\", \n",
    "          \"electronica\", \"electronic\", \"country\", \"classical\", \"classic rock\",\n",
    "          \"classic\", \"choral\", \"blues\", \"alternative rock\", \"alternative\",\n",
    "          \"Progressive rock\", \"House\", \"Hip-hop\"]\n",
    "eras = [\"60s\", \"70s\", \"80s\", \"90s\", \"00s\"]\n",
    "\n",
    "speed_tags = [\"fast\", \"slow\"]\n",
    "feeling_tags = [\"weird\", \"soft\", \"happy\", \"sad\", \"catchy\", \"easy listening\", \"sexy\", \"chillout\", \"beautiful\", \"chill\"]\n",
    "loudness_tags = [\"quiet\", \"loud\"]\n",
    "vibe_tags = [\"ambient\", \"party\", \"dance\", \"Mellow\", \"experimental\"]\n",
    "\n",
    "genre_like_tags = [\"solo\", \"blues\", \"Beat\"]\n",
    "\n",
    "feeling_tags = speed_tags + feeling_tags + loudness_tags + vibe_tags\n",
    "    \n",
    "plt.figure(figsize=(10, 7))\n",
    "show_df = normed_tag_df[feeling_tags].T.copy()\n",
    "show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "sns.heatmap(show_df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce taggram to fit fps\n",
    "musicnn_fps = 62.5\n",
    "#averaging_window = int(musicnn_fps / fps) # == 2 - 30fps\n",
    "averaging_window = int(np.round(musicnn_fps / fps)) # == 3 - 20fps\n",
    "# take step average taggram\n",
    "fps_taggram = normed_tag_df.rolling(averaging_window, min_periods=1, axis=0).mean() \n",
    "fps_taggram = fps_taggram.iloc[::averaging_window, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on using only subset\n",
    "used_tag_df = fps_taggram.copy()\n",
    "if taggram_mode == \"feelings\":    \n",
    "    used_tag_df = used_tag_df[feeling_tags]\n",
    "    \n",
    "# rename some columns\n",
    "rename_dict = {\"sexy\": \"sensual\",\n",
    "               \"party\": \"energetic\",\n",
    "               \"dance\": \"moving\",\n",
    "               \"easy listening\": \"harmonious\",\n",
    "               \"catchy\": \"captivating\"}\n",
    "used_tag_df = used_tag_df.rename(columns=rename_dict)\n",
    "# merge some columns\n",
    "merge_dict = {\"chill\": \"chillout\"}\n",
    "for key in merge_dict:\n",
    "    val = merge_dict[key]\n",
    "    used_tag_df[val] = (used_tag_df[key] + used_tag_df[val]) / 2\n",
    "    del used_tag_df[key]\n",
    "\n",
    "tag_df_means = used_tag_df.mean()\n",
    "used_tag_df[used_tag_df < tag_df_means] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ema(arr, ema_val=0.9):\n",
    "    ema = arr[0]\n",
    "    out = []\n",
    "    for item in arr:\n",
    "        ema = ema * ema_val + item * (1 - ema_val)\n",
    "        out.append(ema)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create clusters\n",
    "if create_clusters:\n",
    "    import sklearn\n",
    "    \n",
    "    clustering_feats = used_tag_df.to_numpy()\n",
    "    # add index to give some time continuity\n",
    "    cluster_time = True\n",
    "    if cluster_time:\n",
    "        idx_arr = np.expand_dims(np.arange(len(clustering_feats)), 1)\n",
    "        idx_arr = idx_arr / idx_arr.sum() * used_tag_df.sum().max() * cluster_time_weight\n",
    "        clustering_feats = np.concatenate([clustering_feats, idx_arr], axis=1)\n",
    "    # create high dim umap embeddings for clustering\n",
    "    cluster_on_umap_high_d = False\n",
    "    clusterable_embedding = np.array(apply_ema(clustering_feats, ema_val=ema_val_clustering))\n",
    "    \n",
    "    if use_k_means:\n",
    "        # cluster        \n",
    "        from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "        # Instantiate the clustering model and visualizer\n",
    "        model = clusterer = sklearn.cluster.KMeans(n_clusters=5, n_init=20, max_iter=500)\n",
    "        visualizer = KElbowVisualizer(\n",
    "            model, k=(min_clusters, max_clusters), metric='calinski_harabasz', #'silhouette', #'calinski_harabasz', \n",
    "            timings=False\n",
    "        )\n",
    "\n",
    "        visualizer.fit(clusterable_embedding)        # Fit the data to the visualizer\n",
    "        visualizer.show()  \n",
    "        num_clusters = visualizer.elbow_value_\n",
    "        # make final clustering\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=num_clusters, n_init=50, max_iter=500)\n",
    "        labels = clusterer.fit_predict(clusterable_embedding)\n",
    "        # determine centers\n",
    "        real_centers = clusterer.cluster_centers_\n",
    "        dist_to_centers = np.array([np.mean((emb - real_centers) ** 2, axis=-1)\n",
    "                                    for emb in clusterable_embedding[:, :]])\n",
    "        dist_to_centers = torch.from_numpy(dist_to_centers)\n",
    "        if cluster_time:\n",
    "            centers = real_centers[:, :-1]\n",
    "        else:\n",
    "            centers = real_centers\n",
    "        \n",
    "    else:\n",
    "        # cluster using hdbscan\n",
    "        from hdbscan import HDBSCAN\n",
    "        min_samples = 36\n",
    "        hdbscan_labels = [-1]\n",
    "        while len(np.unique(hdbscan_labels)) < 3:\n",
    "            hdbscan_clusterer = HDBSCAN(min_samples=min_samples,  #35, \n",
    "                                        cluster_selection_epsilon=0., \n",
    "                                        min_cluster_size=min(100, len(clusterable_embedding) // 10))\n",
    "            hdbscan_labels = hdbscan_clusterer.fit_predict(clusterable_embedding)\n",
    "            min_samples = min_samples // 2\n",
    "        # assign outliers (labels == -1) to the previous non-outlier label \n",
    "        clean_hdbscan_labels = []\n",
    "        last_label = np.array(hdbscan_labels[hdbscan_labels != -1])[0]\n",
    "        for label in hdbscan_labels:\n",
    "            if label == -1:\n",
    "                label = last_label\n",
    "            else:\n",
    "                last_label = label\n",
    "            clean_hdbscan_labels.append(label)\n",
    "        labels = np.array(clean_hdbscan_labels)\n",
    "        num_labels = len(np.unique(labels))\n",
    "\n",
    "        \n",
    "        # assign distances to cluster for each point\n",
    "        from hdbscan_utils import *\n",
    "        data = clusterable_embedding\n",
    "        tree = hdbscan_clusterer.condensed_tree_\n",
    "        exemplar_dict = {c: exemplars(c, tree) for c in tree._select_clusters()}\n",
    "        cluster_ids = tree._select_clusters()\n",
    "        raw_tree = tree._raw_tree\n",
    "        all_possible_clusters = np.arange(data.shape[0], raw_tree['parent'].max() + 1).astype(np.float64)\n",
    "        max_lambda_dict = {c:max_lambda_val(c, raw_tree) for c in all_possible_clusters}\n",
    "\n",
    "        point_dict = {c:set(points_in_cluster(c, raw_tree)) for c in all_possible_clusters}\n",
    "        cluster_distances = np.array([combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n",
    "                                                       max_lambda_dict, point_dict, False) for x in range(len(data))])\n",
    "        dist_to_centers =  1 - torch.from_numpy(cluster_distances)\n",
    "        # find cluster representatives by averagin the points per cluster\n",
    "        real_centers = np.array([data[labels == i].mean(axis=0) for i in range(num_labels)])\n",
    "        if cluster_time:\n",
    "            centers = real_centers[:, :-1]\n",
    "        else:\n",
    "            centers = real_centers\n",
    "\n",
    "    show_2d_umap = 0\n",
    "    if show_2d_umap:\n",
    "        from umap import UMAP\n",
    "        # create 2D UMAP embedding to plot\n",
    "        mapper = UMAP(\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.0,\n",
    "            n_components=2,\n",
    "            random_state=42,\n",
    "            metric=\"cosine\",\n",
    "        ).fit(clustering_feats)\n",
    "        # make plot\n",
    "        import umap.plot\n",
    "        umap.plot.output_notebook()\n",
    "        df = pd.DataFrame({\"step\": list(range(len(labels))),\n",
    "                           \"cluster\": labels,\n",
    "                           })\n",
    "        p = umap.plot.interactive(mapper, \n",
    "                                  labels=df[\"cluster\"], \n",
    "                                  #values = df[\"step\"],\n",
    "                                  hover_data=df, point_size=10)\n",
    "        umap.plot.show(p)\n",
    "    # show clusters over time\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(range(len(labels)), labels, s=1.5)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # show cluster dists over time\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i in range(dist_to_centers.shape[1]):\n",
    "        plt.plot(dist_to_centers[:, i], label=str(i))\n",
    "    l = plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # show heatmap \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    show_df = used_tag_df.T.copy()\n",
    "    show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "    show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# listen to clusters\n",
    "\n",
    "cluster_idx = 0\n",
    "\n",
    "samples_per_step = int(len(song) / len(hdbscan_labels)) + 1\n",
    "frame_assignments = []\n",
    "for label in hdbscan_labels:\n",
    "    frame_assignments.extend([label] * samples_per_step)\n",
    "frame_assignments = np.array(frame_assignments)\n",
    "sections = frame_assignments == cluster_idx\n",
    "song_section = song[sections[:len(song)]]\n",
    "\n",
    "IPython.display.Audio(song_section, rate=sr, autoplay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "#story_idx = max(0 - n_start_prompts, 0)\n",
    "story_idx = 0\n",
    "top_k = dist_to_centers[story_idx].topk(k=gpt_story_top_k, largest=False)\n",
    "print(dist_to_centers[story_idx])\n",
    "print(top_k)\n",
    "print(dist_to_centers[story_idx].max())\n",
    "print((1 - (top_k.values / dist_to_centers[story_idx].sum())) ** 2)\n",
    "#plt.plot((1 - (top_k.values / dist_to_centers[story_idx].max())) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_stories_and_weights(cluster_gpt_stories, n_start_prompts, dist_to_centers, gpt_story_top_k, idx):\n",
    "    if cluster_gpt_stories is not None:\n",
    "        story_idx = max(idx - n_start_prompts, 0)\n",
    "        top_k = dist_to_centers[story_idx].topk(k=gpt_story_top_k, largest=False)\n",
    "        story_weights = (1 - (top_k.values / dist_to_centers[story_idx].max())) ** 2\n",
    "        top_idcs = top_k.indices\n",
    "        gpt_stories = [cluster_gpt_stories[i] for i in top_idcs]\n",
    "    else:\n",
    "        gpt_stories = [\"\"]\n",
    "        story_weights = [1]\n",
    "    return gpt_stories, story_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_tag_df.mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest_classification\n",
    "\n",
    "def filter_theme_labels(series, n=6, threshold=0.5, factor=0.9):\n",
    "    # sort by logits\n",
    "    cluster_theme = series.sort_values(ascending=False)\n",
    "    #print(cluster_theme)\n",
    "    # filter logits below certain value\n",
    "    cluster_theme = cluster_theme[cluster_theme > threshold]\n",
    "    # filter logits that are lower than mean over all\n",
    "    cluster_theme = cluster_theme[[col for col in cluster_theme.index if cluster_theme[col] > factor * used_tag_df.mean()[col]]]\n",
    "    # pick top N\n",
    "    cluster_theme = cluster_theme.iloc[:n]\n",
    "    return cluster_theme\n",
    "\n",
    "\n",
    "center_df = pd.DataFrame(centers, columns=used_tag_df.columns)\n",
    "\n",
    "cluster_themes = []\n",
    "for i in range(len(center_df)):\n",
    "    cluster_theme = filter_theme_labels(center_df.iloc[i])\n",
    "    cluster_vals = round(cluster_theme, 2).to_list()\n",
    "    cluster_theme_names = cluster_theme.index.to_list()\n",
    "    cluster_theme_names = \", \".join(cluster_theme_names).lower()\n",
    "    print(str(i) + \":\",  cluster_theme_names, cluster_vals)\n",
    "    cluster_themes.append(cluster_theme_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_theme = used_tag_df.mean().sort_values(ascending=False).iloc[:5]\n",
    "main_theme_words = \", \".join(main_theme.index.to_list())\n",
    "main_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main distinctive features \n",
    "print(\", \".join(center_df.std().sort_values(ascending=False)[:5].index.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# listen to clusters\n",
    "\n",
    "cluster_idx = 0\n",
    "\n",
    "plt.scatter(range(len(labels)), labels, s=1.5)\n",
    "plt.show()\n",
    "\n",
    "samples_per_step = int(len(song) / len(labels)) + 1\n",
    "\n",
    "frame_assignments = []\n",
    "for label in labels:\n",
    "    frame_assignments.extend([label] * samples_per_step)\n",
    "frame_assignments = np.array(frame_assignments)\n",
    "\n",
    "sections = frame_assignments == cluster_idx\n",
    "\n",
    "song_section = song[sections[:len(song)]]\n",
    "\n",
    "IPython.display.Audio(song_section, rate=sr, autoplay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# create musicnn prompts\n",
    "clip_prompts = []\n",
    "pbar = tqdm(list(used_tag_df.iterrows()))\n",
    "\n",
    "for i, row in pbar:\n",
    "    row = row[row > tag_df_means]\n",
    "    sorted_row = row.sort_values(ascending=False)\n",
    "\n",
    "    # generate clip prompt for current musicnn targets\n",
    "    if prompt_mode == \"top_k\":\n",
    "        # get tags\n",
    "        top_tag_names = list(sorted_row.iloc[:k].index)\n",
    "        #print(top_tag_names)\n",
    "        pbar.set_description(\", \".join(top_tag_names))\n",
    "        clip_prompt = \", \".join(top_tag_names)\n",
    "    elif prompt_mode == \"weighted_top_k\":\n",
    "        sorted_row = filter_theme_labels(sorted_row, n=k)#, factor=0.8, threshold=0.1)\n",
    "        top_tag_names = list(sorted_row.index)\n",
    "        top_tag_vals = list(sorted_row)\n",
    "        clip_prompt = {name: val for name, val in zip(top_tag_names, top_tag_vals)}\n",
    "    elif prompt_mode == \"gpt\":\n",
    "        sorted_row = row.sort_values(ascending=False)\n",
    "        top_tags = sorted_row.iloc[:k]\n",
    "        top_tag_names = list(top_tags.index)\n",
    "        if len(top_tag_names) == 0:\n",
    "            top_tag_names = [\"Undecided emptiness\"]\n",
    "        merged_top_tags = \", \".join(top_tag_names)\n",
    "        if merged_top_tags in prompt_hash_table:\n",
    "            clip_prompt = prompt_hash_table[merged_top_tags]\n",
    "        else:\n",
    "            clip_prompt = gpt_create_prompt(gpt_model, gpt_tokenizer, merged_top_tags)\n",
    "            pbar.set_description(\"Tags: \" + merged_top_tags + \" Prompt: \" + clip_prompt)\n",
    "            #clip_encoding = imagine.create_text_encoding(clip_prompt)\n",
    "            prompt_hash_table[merged_top_tags] = clip_prompt\n",
    "            \n",
    "    clip_prompts.append(clip_prompt)\n",
    "    \n",
    "# how many steps are there to fill at the start of the song (256 is the size of the fft-windows of musicnn)\n",
    "start_prompt = clip_prompts[0]\n",
    "n_start_prompts = int(np.round((len(song) / (256 * averaging_window) - len(used_tag_df))))\n",
    "clip_prompts.extend([start_prompt] * n_start_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../StyleCLIP_modular\")\n",
    "sys.path.append(\"../CLIPGuidance\")\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from style_clip import Imagine, create_text_path\n",
    "\n",
    "net = \"vqgan\" # conv, vqgan\n",
    "\n",
    "args = {}\n",
    "args[\"lr_schedule\"] = 0\n",
    "args[\"seed\"] = 1\n",
    "\n",
    "args[\"neg_text\"] = 'incoherent, confusing, cropped, watermarks'\n",
    "#'text, signature, watermarks, writings, scribblings'#\n",
    "\n",
    "args[\"clip_names\"] = [\"ViT-B/16\", \"ViT-B/32\"]#, \"RN50\"]\n",
    "args[\"averaging_weight\"] = 0\n",
    "args[\"early_stopping_steps\"] = 0\n",
    "args[\"use_tv_loss\"] = 1\n",
    "args[\"lpips_weight\"] = lpips_weight\n",
    "\n",
    "if net == \"vqgan\":\n",
    "    args[\"model_type\"] = \"vqgan\"\n",
    "    args[\"lr\"] = 0.1\n",
    "    \n",
    "elif net == \"conv\":\n",
    "    args[\"model_type\"] = \"conv\"\n",
    "    args[\"act_func\"] = \"gelu\"\n",
    "    args[\"stride\"] = 1\n",
    "    args[\"num_layers\"] = 5\n",
    "    args[\"downsample\"] = False\n",
    "    args[\"norm_type\"] = \"layer\"\n",
    "    args[\"num_channels\"] = 64\n",
    "    args[\"sideX\"] = 1080\n",
    "    args[\"sideY\"] = 720\n",
    "    args[\"lr\"] = 0.005\n",
    "    args[\"stack_size\"] = 4\n",
    "\n",
    "\n",
    "args[\"batch_size\"] = clip_batch_size\n",
    "args[\"sideX\"] = sideX # 688 #624 #544 #480 \n",
    "args[\"sideY\"] = sideY # 384 #352 #304 #272 \n",
    "# 688x384 - 7.792GB, 34s/it\n",
    "# 720x400 - 7.948GB, 41.3s/it - crashes after a bit\n",
    "# 624x352 - 6.9GB, 29.8s/it\n",
    "# 544x304 - 5850MB, 24s/it at 100its per step\n",
    "args[\"circular\"] = 0\n",
    "\n",
    "imagine = Imagine(\n",
    "                save_progress=False,\n",
    "                open_folder=False,\n",
    "                save_video=False,\n",
    "                verbose=False,\n",
    "                **args\n",
    "               )\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip_model = imagine.perceptor.models[0]\n",
    "#gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = \"The following are adjectives describing a song, followed by a description of the corresponding image:\\n \"\n",
    "#prefix = \"The following are adjectives describing an image, listed in the order of importance. They are followed by a full description of the corresponding image:\\n \"\n",
    "#prompter = \". Full description:\"\n",
    "#examples = {\"sad, dark, fast\": \" A man is running through dark woods while crying.\",\n",
    "#            \"sad, beautiful, soft, quiet, slow\": \" An old woman is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "#            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors forming strong geometric shapes that resemble a rave.\",\n",
    "#            \"weird, happy, fast\": \" A man is experiencing a strange dream. He is struggling to feel his feelings, his emotions as they rush too quickly through his body. He is an a state of ecstacy.\",\n",
    "#            \"harmonious, mellow\": \" An electric light begins to dim at a distant point in the sky. You feel complete and at one with your environment.\",\n",
    "#            \"slow, quiet\": \" A lion's roar stops in front of him. The lion is slowly moving forward and approaching you. He is silent.\"\n",
    "#           }\n",
    "\n",
    "#target_text = \"slow, brutal\"\n",
    "#target_clip_feats = clip_model.encode_text(tokenize(target_text).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"An ancient dream starts to calm down. It is quiet and peaceful\"\n",
    "\"An ordinary man appears before you. He is listening to you. He is a crazed crazy crazy crazy crazy crazy. He is beating on your legs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "#             start_text=\"\", p=0.9, \n",
    " ##            prefix=prefix, examples=examples, prompter=prompter, target_text=target_text,\n",
    " #            clip_weight=0.9, \n",
    " #            clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=5)\n",
    "#print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from mustovi_utils import load_gpt_model, gen_sent\n",
    "from clip import tokenize\n",
    "\n",
    "\n",
    "def gpt_create_prompt(cluster_words_list, gpt_name, clip_model, gpt_model=None, gpt_tokenizer=None, gpt_prefix=\"\"):\n",
    "    if gpt_model is None:\n",
    "        gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)\n",
    "\n",
    "    prefix = \"The following are adjectives describing a song, followed by a description of the corresponding image:\\n \"\n",
    "    prefix = \"The following are adjectives describing an image, listed in the order of importance. They are followed by a full description of the corresponding image:\\n \"\n",
    "    prompter = \". Full description:\"\n",
    "    #examples = {\"sad, dark, fast\": \" A man is running through dark woods while crying.\",\n",
    "    #            \"sad, beautiful, soft, quiet, slow\": \" An old woman is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "    #            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors forming strong geometric shapes that resemble a rave.\",\n",
    "    #           }\n",
    "    examples = {\"sad, dark, fast\": \" Running through dark woods while crying.\",\n",
    "            \"sad, beautiful, soft, quiet, slow\": \" An old widow is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors are forming strong geometric shapes that resemble a rave.\",\n",
    "            \"weird, happy, fast\": \" A man is experiencing a strange dream. He is struggling to feel his emotions as they rush too quickly through his body. He is an a state of ecstacy.\",\n",
    "            \"harmonious, mellow\": \" An electric light begins to dim at a distant point in the sky. You feel complete and at one with your environment.\",\n",
    "            \"slow, quiet\": \" A lion's roar stops in front of him. The lion is slowly moving forward and approaching you. He is silent.\"\n",
    "           }\n",
    "\n",
    "    \n",
    "    gpt_stories = []\n",
    "    for target_text in tqdm(cluster_words_list):\n",
    "        target_clip_feats = clip_model.encode_text(tokenize(target_text).to(\"cuda\"))\n",
    "\n",
    "        texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "                 start_text=gpt_prefix, p=0.9, \n",
    "                 prefix=prefix, examples=examples, prompter=prompter, target_text=target_text,\n",
    "                 clip_weight=0.4, \n",
    "                 clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=1)\n",
    "        text = texts[0]\n",
    "        gpt_stories.append(text)\n",
    "        print(text)\n",
    "        print()\n",
    "    gpt_model = gpt_model.to(\"cpu\")\n",
    "    return gpt_stories\n",
    "\n",
    "used_gpt_stories = None\n",
    "if do_create_gpt_cluster_stories:\n",
    "    used_gpt_stories = gpt_create_prompt(cluster_themes, gpt_name, imagine.perceptor.models[0], gpt_prefix=gpt_cluster_prefix)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_gpt_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mustovi_utils import load_gpt_model, gen_sent\n",
    "from clip import tokenize\n",
    "\n",
    "\n",
    "def gpt_create_theme(theme_words, gpt_name, clip_model, gpt_model=None, gpt_tokenizer=None):\n",
    "    if gpt_model is None:\n",
    "        gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)\n",
    "\n",
    "    prefix = \"The following are adjectives, followed by a matching artstyle:\\n \"\n",
    "    prompter = \". Matching artstyle:\"\n",
    "    \n",
    "    prompter = \"The name of a matching painter is:\"\n",
    "    prefix= \"The following are lists of words describing art, followed by the name of the artist:\\n \"\n",
    "    \n",
    "    prefix = \"The following are lists of words describing art, followed by the name of the artist:\\n \"\n",
    "    prompter = \"Matching visual artist:\"\n",
    "    \n",
    "    prefix = \"The following are lists of adjectives, listed in order of importance. They are followed by a name of an artstyle that matches them:\\n \"\n",
    "    prompter = \". Matching artstyle:\"\n",
    "        \n",
    "    examples = {\"introspective, beautiful, sad\": \" A moody, ambient painting.\",\n",
    "                \"expressive, wild, colourful\": \" An expressionist piece of art.\",\n",
    "                \"epic, fantasy, stunning, moody\": \" Illustrated by Greg Rutkowski.\",\n",
    "                \"introspective, beautiful, sad\": \" Impressionism.\",\n",
    "                \"realistic, beautiful, landscapes, forgotten civilizations\": \" By James Gurney.\"}\n",
    "    # popular, internet{prompter} Trending on artstation.\n",
    "    # rendered, detailed, high-quality{prompter} Rendered in unreal engine.\n",
    "    # expressionist, beautiful, vibrant. {prompter} Van Gogh.\n",
    "    # happy, dreamy, romantic, sensual. {prompter} Gustav Klimt.\n",
    "    \n",
    "    target_clip_feats = clip_model.encode_text(tokenize(theme_words).to(\"cuda\"))\n",
    "\n",
    "    texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "                 start_text=\"\", p=0.9, \n",
    "                 prefix=prefix, examples=examples, prompter=prompter, target_text=theme_words,\n",
    "                 clip_weight=0.2, \n",
    "                 clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=5)    \n",
    "    text = texts[0]\n",
    "    print(texts)\n",
    "    gpt_model = gpt_model.to(\"cpu\")\n",
    "    return text\n",
    "\n",
    "gpt_theme = \"\"\n",
    "if create_gpt_artstyle:\n",
    "    gpt_theme = gpt_create_theme(main_theme_words.lower(), gpt_name, imagine.perceptor.models[0])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gpt_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clip_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate encodings based on prompts\n",
    "\n",
    "clip_target_encodings = []\n",
    "clip_feature_hash_table = dict()\n",
    "gpt_suffix = \"\" if len(gpt_theme) == 0 else f\" {gpt_theme}\"\n",
    "\n",
    "count = []\n",
    "\n",
    "def encode(prompt):\n",
    "    prompt = prefix + prompt\n",
    "    if general_theme is not None:\n",
    "        prompt = prompt + general_theme\n",
    "    prompt += gpt_suffix\n",
    "    if prompt in clip_feature_hash_table:\n",
    "        encoding = clip_feature_hash_table[prompt]\n",
    "    else:\n",
    "        count.append(0)\n",
    "        if len(count) % 50 == 0:\n",
    "            print(prompt)\n",
    "        encoding = imagine.create_clip_encoding(text=prompt, img=img_theme)\n",
    "        #encoding = imagine.create_text_encoding(prompt)\n",
    "        clip_feature_hash_table[prompt] = encoding\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def weighted_average_encoding(encodings, weights):\n",
    "    clip_encoding = [norm(torch.stack([norm(encoding[j]) * weight for encoding, weight in zip(encodings, weights)]).sum(dim=0))\n",
    "                         for j in range(len(encodings[0]))]\n",
    "    return clip_encoding\n",
    "\n",
    "\n",
    "def norm(a):\n",
    "    return a / a.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def clip_mean_direction(direction_prompt, base_prompts, imagine):\n",
    "    base_encs = [imagine.create_clip_encoding(text=p) for p in base_prompts]\n",
    "    base_plus_dir_encs = [imagine.create_clip_encoding(text=p + direction_prompt) for p in base_prompts]\n",
    "    diff_encs = [[norm(norm(base_ext_enc[i]) - norm(base_enc[i])) for i in range(len(base_enc))] for base_enc, base_ext_enc in zip(base_encs, base_plus_dir_encs)]\n",
    "    mean_diff_enc = [norm(torch.stack([diff_encs[j][i] for j in range(len(diff_encs))]).mean(dim=0)) for i in range(len(diff_encs[0]))]\n",
    "    return mean_diff_enc\n",
    "\n",
    "\n",
    "\n",
    "if use_mean_dirs:\n",
    "    base_prompts = [\"A photo of \", \" \", \"A painting of \", \"This painting is: \", \"This photo looks \", \"I feel \", \"I feel: \", \"The sky is \",\n",
    "                   \"This is \", \"The ground is \", \"This person is \", \"She is \", \"He  is \", \"A \"]\n",
    "    dir_dict = {col: clip_mean_direction(col, base_prompts, imagine) for col in used_tag_df}\n",
    "\n",
    "\n",
    "for idx, prompt in enumerate(tqdm(clip_prompts)):\n",
    "    gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                             dist_to_centers, gpt_story_top_k, idx)\n",
    "    \n",
    "    story_encodings = []\n",
    "    for gpt_story, story_weight in zip(gpt_stories, story_weights):\n",
    "        if isinstance(prompt, dict):\n",
    "            if use_mean_dirs:\n",
    "                take_avg = True\n",
    "                \n",
    "                clip_encoding = encode(gpt_story + \" \")\n",
    "                \n",
    "                if take_avg:\n",
    "                    dir_encodings = [dir_dict[tag] for tag in prompt]\n",
    "                    weights = list(prompt.values())\n",
    "                    if len(dir_encodings) > 0:                        \n",
    "                        dir_mean_encoding = weighted_average_encoding(dir_encodings, weights)\n",
    "                        clip_encoding = weighted_average_encoding([clip_encoding, dir_mean_encoding], [1 - mood_weight, mood_weight])\n",
    "                else:\n",
    "                    for tag in prompt:\n",
    "                        weight = prompt[tag]\n",
    "                        clip_encoding = [clip_encoding[i] + dir_dict[tag][i] * weight for i in range(len(clip_encoding))]\n",
    "                clip_encoding = [norm(enc) for enc in clip_encoding]\n",
    "            else:\n",
    "                encodings = [encode(gpt_story + \" It feels \" + prompt_key + \".\") for prompt_key in prompt]\n",
    "                weights = list(prompt.values())\n",
    "                clip_encoding = weighted_average_encoding(encodings, weights)\n",
    "        else:\n",
    "            story_prompt = gpt_story + \" \" + prompt + \".\"\n",
    "            clip_encoding = encode(story_prompt)\n",
    "        story_encodings.append(clip_encoding)\n",
    "    clip_encoding = weighted_average_encoding(story_encodings, story_weights)\n",
    "\n",
    "    \n",
    "    clip_encoding = [enc.to(\"cpu\") for enc in clip_encoding]\n",
    "    clip_target_encodings.append(clip_encoding)\n",
    "    \n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test directions\n",
    "\"\"\"\n",
    "print(dir_dict.keys())\n",
    "base_text = \"Home.\"\n",
    "steps = 150\n",
    "text_weight = 0.4\n",
    "dirs = [\"slow\", \"experimental\"]\n",
    "weights = [1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "text_enc = imagine.create_text_encoding(base_text)\n",
    "\n",
    "\n",
    "dir_encodings = [dir_dict[tag] for tag in dirs]\n",
    "print(dir_encodings[0][0][0][:10])\n",
    "dir_mean_encoding = weighted_average_encoding(dir_encodings, weights)\n",
    "clip_encoding = weighted_average_encoding([text_enc, dir_mean_encoding], [text_weight, 1 - text_weight])\n",
    "print(clip_encoding[0][0][:10])\n",
    "\n",
    "#for p, w in zip(dirs, weights):\n",
    "#    dir_enc = dir_dict[p]\n",
    "#    clip_encoding = [norm(text_enc[i] * text_weight + dir_enc[i] * (1 - text_weight)) for i in range(len(dir_enc))]\n",
    "\n",
    "    \n",
    "imagine.set_clip_encoding(encoding=clip_encoding)\n",
    "imagine.reset()\n",
    "for _ in tqdm(range(steps)):\n",
    "    img, loss = imagine.train_step(0, 0)\n",
    "\"\"\"\n",
    "# to_pil(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip_target_encodings[10][0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ema of encodings to smoothen\n",
    "ema_encodings = []\n",
    "ema = clip_target_encodings[0]\n",
    "\n",
    "for encoding in clip_target_encodings:\n",
    "    ema = [ema_val * ema[i].to(\"cpu\") + (1 - ema_val) * encoding[i].to(\"cpu\") for i in range(len(encoding))]\n",
    "    ema_encodings.append(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "from mustovi_utils import get_spec_norm\n",
    "import librosa\n",
    "# create zoom, rotate, shift effects\n",
    "effects = [\"zoom\", \"rotate\", \"shiftX\", \"shiftY\", \"shear\"]\n",
    "harm_effect_dict =dict()  #{\"rotate\": 0.0}\n",
    "perc_effect_dict = dict() #{\"zoom\": -0.5}\n",
    "cqt_effect_dict = [{\"zoom\": 1.0}, \n",
    "                   {\"rotate\": 1.0},\n",
    "                   {\"shiftX\": 1.0}, \n",
    "                   {\"shiftY\": 1.0},\n",
    "                   {\"shiftY\": -1.0},\n",
    "                   {\"shiftX\": -1.0},\n",
    "                   {\"rotate\": -1.0},\n",
    "                   {\"zoom\": -1.0},\n",
    "                  ]\n",
    "# divide song in percussion and harm (might divide in pitches later)\n",
    "song_harm, song_perc = librosa.effects.hpss(song)\n",
    "spec_norm_harm = get_spec_norm(song_harm)\n",
    "spec_norm_perc = get_spec_norm(song_perc)\n",
    "# get cqt spec\n",
    "n_chroma = len(cqt_effect_dict)\n",
    "cqt_spec = librosa.feature.chroma_cqt(y=song, sr=sr,hop_length=256, \n",
    "                                      n_chroma=n_chroma, n_octaves=7, \n",
    "                                      bins_per_octave=n_chroma * 4, norm=None)\n",
    "sns.heatmap(cqt_spec)\n",
    "plt.show()\n",
    "# take window averages to match video fps\n",
    "N = averaging_window\n",
    "spec_norm_harm = np.convolve(spec_norm_harm, np.ones(N) / N , mode='valid')[::N]\n",
    "spec_norm_perc = np.convolve(spec_norm_perc, np.ones(N) /N, mode='valid')[::N]\n",
    "cqt_spec = np.array([np.convolve(cqt_line, np.ones(N) / N, mode='valid')[::N] \n",
    "                     for cqt_line in cqt_spec])\n",
    "# min-max norm\n",
    "spec_norm_harm = (spec_norm_harm - spec_norm_harm.min()) / (spec_norm_harm.max() - spec_norm_harm.min())\n",
    "spec_norm_perc = (spec_norm_perc - spec_norm_perc.min()) / (spec_norm_perc.max() - spec_norm_perc.min())\n",
    "cqt_spec = (cqt_spec - cqt_spec.min()) / (cqt_spec.max() - cqt_spec.min())\n",
    "# create effects\n",
    "class Effect:\n",
    "    def __init__(self, strength, zoom=0, rotate=0, \n",
    "                 shiftX=0, shiftY=0, shear=0):\n",
    "        max_zoom = 0.22\n",
    "        self.zoom = 1 + max_zoom * zoom * strength\n",
    "        max_rotate = 10\n",
    "        self.rotate = max_rotate * rotate * strength\n",
    "        max_shift = 12\n",
    "        self.shift_x = max_shift * shiftX * strength\n",
    "        self.shift_y = max_shift * shiftY * strength\n",
    "        \n",
    "        self.shear = 0\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        # transform it\n",
    "        transformed_img = T.functional.affine(img, \n",
    "                                          angle=self.rotate, \n",
    "                                          translate=(self.shift_x,\n",
    "                                                     self.shift_y), \n",
    "                                          scale=self.zoom, \n",
    "                                          shear=self.shear,\n",
    "                                          fill=0,\n",
    "                                          interpolation=torchvision.transforms.InterpolationMode.BILINEAR\n",
    "                                         )\n",
    "        # fill in zeros with nearest neighbor\n",
    "        data = transformed_img.numpy()\n",
    "        mask = np.where(~(data == 0))\n",
    "        interp = NearestNDInterpolator(np.transpose(mask), data[mask])\n",
    "        image_result = interp(*np.indices(data.shape))\n",
    "        return torch.from_numpy(image_result)\n",
    "        \n",
    "\n",
    "def merge_dicts(effect_dict, effect_strength_dict, amplitude):\n",
    "    for key in effect_strength_dict:\n",
    "        content = effect_strength_dict[key] * amplitude\n",
    "        if key in effect_dict:\n",
    "            effect_dict[key] += content\n",
    "        else:\n",
    "            effect_dict[key] = content\n",
    "\n",
    "# create effects that directly alter the image\n",
    "if args[\"model_type\"] == \"vqgan\" or args[\"model_type\"] == \"image\":\n",
    "    effects_list = []\n",
    "    for i in range(len(spec_norm_harm)):\n",
    "        harm = spec_norm_harm[i]\n",
    "        perc = spec_norm_perc[i]\n",
    "        cqt = cqt_spec[:, i]\n",
    "\n",
    "        effect_dict = {}\n",
    "        merge_dicts(effect_dict, harm_effect_dict, harm)\n",
    "        merge_dicts(effect_dict, perc_effect_dict, perc)\n",
    "        for cqt_effect, cqt_amplitude in zip(cqt_effect_dict, cqt):\n",
    "            merge_dicts(effect_dict, cqt_effect, cqt_amplitude)\n",
    "\n",
    "        effect = Effect(total_effect_strength, **effect_dict)\n",
    "        effects_list.append([effect])\n",
    "else:\n",
    "    # create effects that alter the clip target shortly\n",
    "    # these effects should have a different name\n",
    "    effects_list = [[] * len(spec_norm_harm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-bloom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ema_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(effects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4eb652-7a66-4d8a-9b2f-749dce949eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(a):\n",
    "    min_ = a.min()\n",
    "    return (a - min_) / (a.max() - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec7729-1f3f-4d61-ac6b-6cae4d209653",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_substeps = 10\n",
    "max_substeps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9b502-3240-48b5-8d94-2fa77cda4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "substeps_per_step = [sub_steps] * len(ema_encodings)\n",
    "if use_variable_substeps:\n",
    "    # vary number of substeps to take depending on amplitude\n",
    "    spec_norm = get_spec_norm(song)\n",
    "    spec_norm = minmax(np.array(apply_ema(spec_norm, 0.9)))\n",
    "    substeps_per_step = spec_norm * (max_substeps - min_substeps) + min_substeps \n",
    "    substeps_per_step = np.round(substeps_per_step).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744be389-7adb-4633-9449-fade04150788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(substeps_per_step.mean())\n",
    "plt.plot(substeps_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "if len(ema_encodings) > len(effects_list):\n",
    "    ema_encodings = ema_encodings[:-1]\n",
    "if len(ema_encodings) < len(effects_list):\n",
    "    effects_list = effects_list[:-1]\n",
    "assert len(ema_encodings) == len(effects_list), f\"{len(ema_encodings)}, {len(effects_list)}\"\n",
    "\n",
    "\n",
    "img_latents = []\n",
    "save_folder = base_folder + \"tmp/vid_latents/\"\n",
    "if os.path.exists(save_folder):\n",
    "    shutil.rmtree(save_folder)\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "imagine.to(\"cuda\")\n",
    "imagine.reset()\n",
    "imagine.set_clip_encoding(encoding=[item.to(imagine.device) for item in ema_encodings[0]])\n",
    "img, loss = imagine.train_step(0, 0)\n",
    "img = img.detach().cpu()\n",
    "\n",
    "pbar = tqdm(list(range(len(ema_encodings))))\n",
    "for i in pbar:\n",
    "    clip_encoding, effects = ema_encodings[i], effects_list[i]\n",
    "    # apply effects\n",
    "    transformed_img = img.float() \n",
    "    if img is not None and len(effects) > 0:\n",
    "        for effect in effects:\n",
    "            transformed_img = effect(transformed_img.cpu())\n",
    "        transformed_img_normed = transformed_img.mul(2).sub(1).to(imagine.device)\n",
    "        latent, _, [_, _, indices] = imagine.model.model.model.encode(transformed_img_normed)\n",
    "        imagine.set_latent(latent)\n",
    "    # set target encoding in CLIP\n",
    "    clip_encoding = [part.to(imagine.device) for part in clip_encoding]\n",
    "    imagine.set_clip_encoding(encoding=clip_encoding)\n",
    "    # optimize for some steps\n",
    "    for _ in range(substeps_per_step[i]):\n",
    "        img, loss = imagine.train_step(0, 0, lpips_img=transformed_img.to(imagine.device))\n",
    "    img = img.detach().cpu()\n",
    "    # get latent of img\n",
    "    latent = imagine.model.model.get_latent().detach().cpu()\n",
    "    if save_folder is None:\n",
    "        img_latents.append(latent)\n",
    "    else:\n",
    "        latent_name = str(i) + \".pt\"\n",
    "        torch.save(latent, save_folder + latent_name)\n",
    "    # save final img\n",
    "    if i % (len(ema_encodings) // 20) == 0:\n",
    "        pil_img = to_pil(img.squeeze())\n",
    "        display(pil_img)\n",
    "\n",
    "#img_latents = sequential_gen(ema_encodings, effects_list)\n",
    "#img_latents = parallel_gen(clip_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ema_disk(in_folder, out_folder, ema_val=0.9):\n",
    "    if os.path.exists(out_folder):\n",
    "        shutil.rmtree(out_folder)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    \n",
    "    num_items = len([item for item in os.listdir(in_folder) if item.endswith(\".pt\")])\n",
    "    ema = torch.load(in_folder + \"0.pt\")#arr[0]\n",
    "    out = []\n",
    "    for item_idx in range(num_items):\n",
    "        item_name = str(item_idx) + \".pt\"\n",
    "        # load\n",
    "        item = torch.load(in_folder + item_name)\n",
    "        # calc\n",
    "        ema = ema * ema_val + item * (1 - ema_val)\n",
    "        # store\n",
    "        torch.save(ema, out_folder + item_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ema of encodings to smoothen\n",
    "in_folder = base_folder + \"tmp/vid_latents/\"\n",
    "out_folder = base_folder + \"tmp/vid_ema_latents/\"\n",
    "ema_latents = apply_ema_disk(in_folder, out_folder, ema_val=ema_val_latent)\n",
    "#ema_latents = apply_ema(img_latents, ema_val=ema_val_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate between latents to increase fps and make video smoother\n",
    "from mustovi_utils import slerp\n",
    "\n",
    "def boost_frames(ema_latents, boost_fps, song):\n",
    "    goal_frame_count = boost_fps * len(song) / 16000\n",
    "    current_frame_count = len(ema_latents)\n",
    "    frames_to_add = np.ceil(goal_frame_count / current_frame_count)\n",
    "    if frames_to_add > 1:\n",
    "        video_latents = []\n",
    "        for i, latent in enumerate(ema_latents):\n",
    "            if i + 1 == len(ema_latents):\n",
    "                next_latent = ema_latents[i + 1]\n",
    "            else:\n",
    "                next_latent = ema_latents[i + 1]\n",
    "            latents_to_add = [slerp(latent, next_latent, frac) \n",
    "                              for frac in np.arange(frames_to_add) / frames_to_add]\n",
    "            video_latents.extend(latents_to_add)\n",
    "        return video_latents\n",
    "    else:\n",
    "        return ema_latents\n",
    "    \n",
    "def boost_frames_disk(in_folder, out_folder, boost_fps, song):\n",
    "    if os.path.exists(out_folder):\n",
    "        shutil.rmtree(out_folder)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    \n",
    "    num_items = len([item for item in os.listdir(in_folder) if item.endswith(\".pt\")])\n",
    "    \n",
    "    goal_frame_count = boost_fps * len(song) / 16000\n",
    "    current_frame_count = num_items\n",
    "    frames_to_add = np.ceil(goal_frame_count / current_frame_count)\n",
    "    if frames_to_add > 1:\n",
    "        video_latents = []\n",
    "        count = 0\n",
    "        for i in tqdm(range(num_items)):\n",
    "            item_name = str(i) + \".pt\"\n",
    "            latent = torch.load(in_folder + item_name)\n",
    "            if i + 1 == num_items:\n",
    "                next_latent = latent#ema_latents[i]\n",
    "            else:\n",
    "                next_latent_name = str(i + 1) + \".pt\"\n",
    "                next_latent = torch.load(in_folder + item_name)\n",
    "            latents_to_add = [slerp(latent, next_latent, frac) \n",
    "                              for frac in np.arange(frames_to_add) / frames_to_add]\n",
    "            for new_latent in latents_to_add:\n",
    "                item_name = str(count) + \".pt\"\n",
    "                torch.save(new_latent, out_folder + item_name)\n",
    "                count += 1\n",
    "    else:\n",
    "        # rename in folder to out folder\n",
    "        os.rename(input_folder, output_folder)\n",
    "        \n",
    "#video_latents = boost_fps(ema_latents, boost_fps, song)\n",
    "in_folder = base_folder + \"tmp/vid_ema_latents/\"\n",
    "out_folder = base_folder + \"tmp/vid_boosted_latents/\"\n",
    "boost_frames_disk(in_folder, out_folder, boost_fps, song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "import soundfile\n",
    "import moviepy.editor as mpy\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_img_paths(root):\n",
    "    paths = [os.path.join(root, f) for f in os.listdir(root)\n",
    "        if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "    paths = sorted(paths, key= lambda x: int(x.split(\"/\")[-1].split(\"_\")[0].split(\".\")[0]))\n",
    "    return paths\n",
    "\n",
    "\n",
    "def create_video(imgs, song, song_name, vid_name, sr, duration, bitrate=\"5000k\"):\n",
    "    os.makedirs(\"video_gens\", exist_ok=True)\n",
    "    video_path = f\"video_gens/{vid_name}\"\n",
    "    \n",
    "    if isinstance(imgs, list):\n",
    "        img_len = len(imgs)\n",
    "    else:\n",
    "        img_len = len([img for img in os.listdir(imgs) \n",
    "                       if img.endswith(\".jpg\") or img.endswith(\".png\")])\n",
    "    \n",
    "    audio = mpy.AudioFileClip(song_name, fps=sr, nbytes=4)  \n",
    "    if duration is not None:\n",
    "        audio.set_duration(duration)\n",
    "    #from moviepy.audio.AudioClip import AudioArrayClip\n",
    "    #audio = AudioArrayClip(np.expand_dims(song, axis=0), fps=sr) # from a numerical array\n",
    "    #audio = AudioArrayClip(np.expand_dims(song, axis=1), fps=sr)\n",
    "    \n",
    "    # Write temporary audio file\n",
    "    #soundfile.write('tmp.wav', song, sr)\n",
    "    #audio = mpy.AudioFileClip(\"tmp.wav\", fps=sr)\n",
    "    \n",
    "    # Generate final video\n",
    "    vid_fps = len(imgs) / audio.duration\n",
    "    video = mpy.ImageSequenceClip(imgs, fps=vid_fps)\n",
    "    video = video.set_audio(audio)\n",
    "    \n",
    "    video.write_videofile(video_path, \n",
    "                      codec=\"libx264\",\n",
    "                      fps=vid_fps,\n",
    "                      #audio_codec=\"libmp3lame\",\n",
    "                      audio_codec=\"aac\",\n",
    "                      threads=10,\n",
    "                      bitrate=bitrate,\n",
    "                      audio_bitrate=\"320k\",\n",
    "                      preset=\"slow\",\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.to(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create images and save to disk:\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "extension = \".jpg\"\n",
    "tmp_folder = base_folder + \"tmp/vid_imgs\" \n",
    "final_latents_folder = base_folder + \"tmp/vid_boosted_latents/\"\n",
    "\n",
    "if os.path.exists(tmp_folder):\n",
    "    shutil.rmtree(tmp_folder)\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "\n",
    "gen_model = imagine.model.model\n",
    "imagine.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "gen_model.to(device)\n",
    "\n",
    "\n",
    "num_items = len([item for item in os.listdir(final_latents_folder) if item.endswith(\".pt\")])\n",
    "for i in tqdm(range(num_items)):\n",
    "    latent = torch.load(final_latents_folder + str(i) + \".pt\")\n",
    "#for i, latent in enumerate(tqdm(video_latents)):\n",
    "    img = gen_model(latents=latent.to(device)).to(\"cpu\")\n",
    "    pil_img = to_pil(img.squeeze())\n",
    "    pil_img.save(os.path.join(tmp_folder, f\"{i}{extension}\"), subsampling=0, quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%m_%d_%H:%M\")  #(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "video_name = f\"{song_name.split('/')[-1].split('.')[0]}_{prompt_mode}_ema{ema_val}_steps{sub_steps}\"\n",
    "video_name += \"_\"+ gpt_theme.replace(\" \", \"_\") if create_gpt_artstyle else \"\"\n",
    "video_name += f\"_{args['model_type']}_{date_time}.mp4\"\n",
    "#video_name = \"loud_pipes_kiefer.mp4\"\n",
    "\n",
    "paths = load_img_paths(base_folder + \"tmp/vid_imgs\")\n",
    "create_video(paths, raw_song, song_name, video_name, old_sr, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
    "if not os.path.exists(\"Real-ESRGAN\"):\n",
    "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "    %cd Real-ESRGAN\n",
    "    # Set up the environment\n",
    "    !$PYTHONPATH -m pip install basicsr facexlib gfpgan\n",
    "    !$PYTHONPATH -m pip install -r requirements.txt\n",
    "    !$PYTHONPATH setup.py develop\n",
    "    # Download the pre-trained model\n",
    "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
    "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth -P experiments/pretrained_models\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Real-ESRGAN\")\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs(imgs, out_folder=None, scale=4, tile=0):\n",
    "    \n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    upsampler = RealESRGANer(\n",
    "        scale=scale,\n",
    "        model_path=\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\",\n",
    "        model=model,\n",
    "        tile=tile,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=1)\n",
    "    \n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            img = np.array(Image.open(img))[:,:,::-1]\n",
    "        \n",
    "        output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs\n",
    "\n",
    "\n",
    "import torchvision\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs_custom(imgs, out_folder=None, scale=4, tile=0):\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    loadnet = torch.load(\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\")\n",
    "    if 'params_ema' in loadnet:\n",
    "        keyname = 'params_ema'\n",
    "    else:\n",
    "        keyname = 'params'\n",
    "    model.load_state_dict(loadnet[keyname], strict=True)\n",
    "    model.eval().cuda().half()\n",
    "\n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            #img = torch.from_numpy(np.ascontiguousarray(Image.open(img))[:,:,::-1].copy()).unsqueeze(0)\n",
    "            img = torch.from_numpy(np.transpose(np.array(Image.open(img))[:,:,::-1].copy(), (2, 0, 1))).float().unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        output = model(img.half()).cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        #output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        output = (output * 255.0).round().astype(np.uint8)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def load_images_from_mp4(path):\n",
    "    vid = imageio.get_reader(path,  'ffmpeg')\n",
    "    imgs = [np.array(image) for image in vid.iter_data()]\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "upscale = True\n",
    "\n",
    "if upscale:\n",
    "    input_folder = base_folder + \"tmp/vid_imgs\"\n",
    "    output_folder = base_folder + \"tmp/upscaled_vid_imgs\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"Upscaling...\")\n",
    "    \n",
    "    #video_path = \"video_gens/any_colour_you_like_pink_floyd_hd_studio_quality_7032261705832661515_top_k_ema0.2_steps100Weird_and_beautiful._vqgan_11_20_14:02.mp4\"\n",
    "    #video_name = video_path.split(\"/\")[0]\n",
    "    #input_paths = load_images_from_mp4(video_name)\n",
    "    \n",
    "    input_paths = load_img_paths(input_folder)\n",
    "    upscale_imgs(input_paths, out_folder=output_folder, scale=4)\n",
    "    #!CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth --input $input_folder --output $output_folder  --netscale 4 --outscale 4 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    upscaled_video_name = video_name.split(\"/\")\n",
    "    upscaled_video_name[-1] = \"HD_\" + upscaled_video_name[-1]\n",
    "    upscaled_video_name = \"/\".join(upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)    \n",
    "    create_video(paths, raw_song, song_name, upscaled_video_name, old_sr, duration, bitrate=\"12000k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if twice_upscale:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    input_folder = base_folder + \"tmp/upscaled_vid_imgs\"\n",
    "    output_folder = base_folder + \"tmp/twice_upscaled_vid_imgs\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)    \n",
    "    print(\"Upscaling...\")\n",
    "    input_paths = load_img_paths(input_folder)\n",
    "    upscale_imgs(input_paths, out_folder=output_folder, scale=4)\n",
    "    #!CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x2plus.pth --input  $input_folder --output $output_folder  --netscale 2 --outscale 2 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    twice_upscaled_video_name = upscaled_video_name.split(\"/\")\n",
    "    twice_upscaled_video_name[-1] = \"Full\" + twice_upscaled_video_name[-1]\n",
    "    twice_upscaled_video_name = \"/\".join(twice_upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)\n",
    "    create_video(paths, song_name, twice_upscaled_video_name, old_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

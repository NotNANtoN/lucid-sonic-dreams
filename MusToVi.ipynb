{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-nursing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "PYTHONPATH = sys.executable\n",
    "\n",
    "# set song\n",
    "song_name = \"free_songs/broke_for_free_by_night_owl.mp3\n",
    "#song_name = \"songs/gnossi_1.mp3\"\n",
    "#song_name = \"songs/henne_song.mp3\"\n",
    "#song_name = \"songs/Präludium_1_bach.mp3\"\n",
    "#song_name = \"songs/Präludium_2_Bach.mp3\"\n",
    "#song_name = \"songs/Blumen.wav\"\n",
    "#song_name = \"songs/Misty.mp3\"\n",
    "#song_name = \"songs/space_is_the_place_men_I_trust.mp3\"\n",
    "#song_name = \"songs/opus_men_I_trust.mp3\"\n",
    "#song_name = \"songs/any_colour_you_like_pink_floyd_hd_studio_quality_7032261705832661515.mp3\"\n",
    "\n",
    "# set some parameters based on how much memory is available\n",
    "gpu_avail = 8\n",
    "\n",
    "sideX, sideY = 688, 384\n",
    "clip_batch_size = 4\n",
    "gpt_name = \"gpt2\"\n",
    "if gpu_avail >= 8:\n",
    "    clip_batch_size = 4\n",
    "    gpt_name = \"neo1.3\"\n",
    "elif gpu_avail >= 12:\n",
    "    clip_batch_size = 4\n",
    "    gpt_name = \"neo2.7\"\n",
    "    sideX, sideY = 880, 484\n",
    "# general params\n",
    "fps = 15\n",
    "boost_fps = 60\n",
    "# musicnn params\n",
    "input_overlap = 1 / 30\n",
    "input_length = 3\n",
    "# song params\n",
    "offset = 0\n",
    "duration = None\n",
    "# post processing params\n",
    "upscale = True\n",
    "twice_upscale = False\n",
    "# video params\n",
    "total_effect_strength = 0.6\n",
    "ema_val = 0.2 # 0.99 is too strong\n",
    "ema_val_latent = 0.25\n",
    "sub_steps = 30\n",
    "lpips_weight = 2.5  # 1.5 is good for 300 sub_steps but more could work too.\n",
    "# prompts\n",
    "base_img_path = \"../CLIP_playground/base_images/\"\n",
    "# settings for prompts\n",
    "k = 5\n",
    "prompt_mode = \"weighted_top_k\" # top_k, weighted_top_k, gpt\n",
    "use_mean_dirs = True\n",
    "avg_weight = 0.35\n",
    "\n",
    "taggram_mode = \"feelings\" # full, feelings\n",
    "prefix = \"\"\n",
    "general_theme = \"\" #\". In the style of James Gurney.\"\n",
    "create_gpt_artstyle = True\n",
    "num_themes = 10 # num GPT themes to sample\n",
    "create_clusters = True\n",
    "do_create_gpt_cluster_stories = True\n",
    "size_gpt_story_selection = 10  # how many stories are created and later selected by CLIP\n",
    "min_clusters = 3\n",
    "max_clusters = 13\n",
    "ema_val_clustering = 0.96\n",
    "\n",
    "gpt_story_top_k = 2  # number of k top cluster stories that will be consideren in CLIP guidance\n",
    "img_theme = None # base_img_path + \"hot-dog.jpg\"\n",
    "# base_img_path + \"Autumn_1875_Frederic_Edwin_Church.jpg\"\n",
    "#\". By Pete Mohrbacher.\"\n",
    "#\". In the style of 'The Persistence of Memory' by Dali.\" #\" by madziowa_p.\" #\" by Jan Brueghel the Elder.\"  #\" by Salvador Dali.\" #\" by Greg Rutkowski.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-thomas",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "resampled_path = \"tmp/resampled.wav\"\n",
    "os.makedirs(\"tmp\", exist_ok=True)\n",
    "\n",
    "# load song and resample to 16k Hz\n",
    "sr = 16000\n",
    "raw_song, old_sr = librosa.load(song_name, offset=offset, duration=duration)\n",
    "song = librosa.resample(raw_song, old_sr, sr)\n",
    "soundfile.write(resampled_path, song, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mustovi_utils import get_taggram\n",
    "    \n",
    "tag_dfs_folder = \"./tmp/tag_dfs\"\n",
    "os.makedirs(tag_dfs_folder, exist_ok=True)\n",
    "key_song_name = song_name.split(\"/\")[-1].split(\".\")[0]\n",
    "tag_df_name = f\"{key_song_name}_{input_length}_{int(1 / input_overlap)}_{offset}_{duration}.csv\"\n",
    "tag_df_path = os.path.join(tag_dfs_folder, tag_df_name)\n",
    "if os.path.exists(tag_df_path):\n",
    "    normed_tag_df = pd.read_csv(tag_df_path, index_col=0)\n",
    "else:\n",
    "    normed_tag_df = get_taggram(resampled_path, input_overlap, input_length)\n",
    "    normed_tag_df.to_csv(tag_df_path)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# ordered by importance\n",
    "show_all_labels = False\n",
    "if show_all_labels:\n",
    "    plt.figure(figsize=(12, 20))\n",
    "    show_df = normed_tag_df.T.copy()\n",
    "    show_df[\"mean\"] = normed_tag_df.mean(axis=0)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = [\"violin\", \"strings\", \"sitar\", \"piano\", \"harpsichord\", \n",
    "               \"harp\", \"guitar\", \"drums\", \"flute\", \"synth\", \"cello\"]\n",
    "genres = [\"techno\", \"soul\", \"rock\", \"rnb\", \"punk\", \"pop\", \"opera\", \n",
    "          \"oldies\", \"new age\", \"metal\", \"jazz\", \"indie rock\", \"indie pop\",\n",
    "          \"indie\", \"indian\", \"heavy metal\", \"hard rock\", \"funk\", \"folk\", \n",
    "          \"electronica\", \"electronic\", \"country\", \"classical\", \"classic rock\",\n",
    "          \"classic\", \"choral\", \"blues\", \"alternative rock\", \"alternative\",\n",
    "          \"Progressive rock\", \"House\", \"Hip-hop\"]\n",
    "eras = [\"60s\", \"70s\", \"80s\", \"90s\", \"00s\"]\n",
    "\n",
    "speed_tags = [\"fast\", \"slow\"]\n",
    "feeling_tags = [\"weird\", \"soft\", \"happy\", \"sad\", \"catchy\", \"easy listening\", \"sexy\", \"chillout\", \"beautiful\", \"chill\"]\n",
    "loudness_tags = [\"quiet\", \"loud\"]\n",
    "vibe_tags = [\"ambient\", \"party\", \"dance\", \"Mellow\", \"experimental\"]\n",
    "\n",
    "genre_like_tags = [\"solo\", \"blues\", \"Beat\"]\n",
    "\n",
    "feeling_tags = speed_tags + feeling_tags + loudness_tags + vibe_tags\n",
    "    \n",
    "plt.figure(figsize=(10, 7))\n",
    "show_df = normed_tag_df[feeling_tags].T.copy()\n",
    "show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "sns.heatmap(show_df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce taggram to fit fps\n",
    "musicnn_fps = 62.5\n",
    "#averaging_window = int(musicnn_fps / fps) # == 2 - 30fps\n",
    "averaging_window = int(np.round(musicnn_fps / fps)) # == 3 - 20fps\n",
    "# take step average taggram\n",
    "fps_taggram = normed_tag_df.rolling(averaging_window, min_periods=1, axis=0).mean() \n",
    "fps_taggram = fps_taggram.iloc[::averaging_window, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on using only subset\n",
    "used_tag_df = fps_taggram.copy()\n",
    "if taggram_mode == \"feelings\":    \n",
    "    used_tag_df = used_tag_df[feeling_tags]\n",
    "    \n",
    "# rename some columns\n",
    "rename_dict = {\"sexy\": \"sensual\",\n",
    "               \"party\": \"energetic\",\n",
    "               \"dance\": \"moving\",\n",
    "               \"easy listening\": \"harmonious\",\n",
    "               \"catchy\": \"captivating\"}\n",
    "used_tag_df = used_tag_df.rename(columns=rename_dict)\n",
    "# merge some columns\n",
    "merge_dict = {\"chill\": \"chillout\"}\n",
    "for key in merge_dict:\n",
    "    val = merge_dict[key]\n",
    "    used_tag_df[val] = (used_tag_df[key] + used_tag_df[val]) / 2\n",
    "    del used_tag_df[key]\n",
    "\n",
    "tag_df_means = used_tag_df.mean()\n",
    "used_tag_df[used_tag_df < tag_df_means] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ema(arr, ema_val=0.9):\n",
    "    ema = arr[0]\n",
    "    out = []\n",
    "    for item in arr:\n",
    "        ema = ema * ema_val + item * (1 - ema_val)\n",
    "        out.append(ema)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters\n",
    "if create_clusters:\n",
    "    import sklearn\n",
    "    \n",
    "    clustering_feats = used_tag_df.to_numpy()\n",
    "    # add index to give some time continuity\n",
    "    cluster_time = True\n",
    "    if cluster_time:\n",
    "        idx_arr = np.expand_dims(np.arange(len(clustering_feats)), 1)\n",
    "        idx_arr = idx_arr / idx_arr.sum() * used_tag_df.sum().max() * 2.5\n",
    "        clustering_feats = np.concatenate([clustering_feats, idx_arr], axis=1)\n",
    "    # create high dim umap embeddings for clustering\n",
    "    cluster_on_umap_high_d = False\n",
    "    clusterable_embedding = np.array(apply_ema(clustering_feats, ema_val=ema_val_clustering))\n",
    "    \n",
    "    # cluster        \n",
    "    from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "    # Instantiate the clustering model and visualizer\n",
    "    model = clusterer = sklearn.cluster.KMeans(n_clusters=5, n_init=20, max_iter=500)\n",
    "    visualizer = KElbowVisualizer(\n",
    "        model, k=(min_clusters, max_clusters), metric='calinski_harabasz', #'silhouette', #'calinski_harabasz', \n",
    "        timings=False\n",
    "    )\n",
    "\n",
    "    visualizer.fit(clusterable_embedding)        # Fit the data to the visualizer\n",
    "    visualizer.show()  \n",
    "    num_clusters = visualizer.elbow_value_\n",
    "\n",
    "\n",
    "    clusterer = sklearn.cluster.KMeans(n_clusters=num_clusters, n_init=50, max_iter=500)\n",
    "    labels = clusterer.fit_predict(clusterable_embedding)\n",
    "\n",
    "\n",
    "\n",
    "    real_centers = clusterer.cluster_centers_\n",
    "\n",
    "    dist_to_centers = np.array([np.mean((emb - real_centers) ** 2, axis=-1)\n",
    "                                for emb in clusterable_embedding[:, :]])\n",
    "    dist_to_centers = torch.from_numpy(dist_to_centers)\n",
    "\n",
    "    if cluster_time:\n",
    "        centers = real_centers[:, :-1]\n",
    "    else:\n",
    "        centers = real_centers\n",
    "\n",
    "    show_2d_umap = 0\n",
    "    if show_2d_umap:\n",
    "        from umap import UMAP\n",
    "        # create 2D UMAP embedding to plot\n",
    "        mapper = UMAP(\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.0,\n",
    "            n_components=2,\n",
    "            random_state=42,\n",
    "            metric=\"cosine\",\n",
    "        ).fit(clustering_feats)\n",
    "        # make plot\n",
    "        import umap.plot\n",
    "        umap.plot.output_notebook()\n",
    "        df = pd.DataFrame({\"step\": list(range(len(labels))),\n",
    "                           \"cluster\": labels,\n",
    "                           })\n",
    "        p = umap.plot.interactive(mapper, \n",
    "                                  labels=df[\"cluster\"], \n",
    "                                  #values = df[\"step\"],\n",
    "                                  hover_data=df, point_size=10)\n",
    "        umap.plot.show(p)\n",
    "    # show clusters over time\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(range(len(labels)), labels, s=1.5)\n",
    "    plt.show()\n",
    "    # show heatmap \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    show_df = used_tag_df.T.copy()\n",
    "    show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "    show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_stories_and_weights(cluster_gpt_stories, n_start_prompts, dist_to_centers, gpt_story_top_k, idx):\n",
    "    if cluster_gpt_stories is not None:\n",
    "        story_idx = max(idx - n_start_prompts, 0)\n",
    "        top_k = dist_to_centers[story_idx].topk(k=gpt_story_top_k, largest=False)\n",
    "        story_weights = (1 - (top_k.values / dist_to_centers[story_idx].max())) ** 2\n",
    "        top_idcs = top_k.indices\n",
    "        gpt_stories = [cluster_gpt_stories[i] for i in top_idcs]\n",
    "    else:\n",
    "        gpt_stories = [\"\"]\n",
    "        story_weights = [1]\n",
    "    return gpt_stories, story_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_k_means = True\n",
    "if not use_k_means:\n",
    "    from hdbscan_utils import *\n",
    "\n",
    "    data = clusterable_embedding\n",
    "    \n",
    "    print(\"1\")\n",
    "    tree = clusterer.condensed_tree_\n",
    "    exemplar_dict = {c: exemplars(c, tree) for c in tree._select_clusters()}\n",
    "    cluster_ids = tree._select_clusters()\n",
    "    \n",
    "    raw_tree = tree._raw_tree\n",
    "    all_possible_clusters = np.arange(data.shape[0], raw_tree['parent'].max() + 1).astype(np.float64)\n",
    "    max_lambda_dict = {c:max_lambda_val(c, raw_tree) for c in all_possible_clusters}\n",
    "                       \n",
    "    point_dict = {c:set(points_in_cluster(c, raw_tree)) for c in all_possible_clusters}\n",
    "    \n",
    "    x = 0\n",
    "    membership_vector = combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n",
    "                                                   max_lambda_dict, point_dict, False)\n",
    "    print(membership_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest_classification\n",
    "\n",
    "def filter_theme_labels(series, n=10, threshold=0.5, factor=1.2):\n",
    "    # sort by logits\n",
    "    cluster_theme = series.sort_values(ascending=False)\n",
    "    # filter logits below certain value\n",
    "    cluster_theme = cluster_theme[cluster_theme > threshold]\n",
    "    # filter logits that are lower than mean over all\n",
    "    cluster_theme = cluster_theme[[col for col in cluster_theme.index if cluster_theme[col] > factor * used_tag_df.mean()[col]]]\n",
    "    # pick top N\n",
    "    cluster_theme = cluster_theme.iloc[:n]\n",
    "    return cluster_theme\n",
    "\n",
    "\n",
    "center_df = pd.DataFrame(centers, columns=used_tag_df.columns)\n",
    "\n",
    "cluster_themes = []\n",
    "for i in range(len(center_df)):\n",
    "    cluster_theme = filter_theme_labels(center_df.iloc[i])\n",
    "    cluster_vals = round(cluster_theme, 2).to_list()\n",
    "    cluster_theme_names = cluster_theme.index.to_list()\n",
    "    cluster_theme_names = \", \".join(cluster_theme_names).lower()\n",
    "    print(str(i) + \":\",  cluster_theme_names, cluster_vals)\n",
    "    cluster_themes.append(cluster_theme_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_theme = used_tag_df.mean().sort_values(ascending=False).iloc[:5]\n",
    "main_theme_words = \", \".join(main_theme.index.to_list())\n",
    "main_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main distinctive features \n",
    "print(\", \".join(center_df.std().sort_values(ascending=False)[:5].index.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# listen to clusters\n",
    "\n",
    "cluster_idx = 2\n",
    "\n",
    "plt.scatter(range(len(labels)), labels, s=1.5)\n",
    "plt.show()\n",
    "\n",
    "samples_per_step = int(len(song) / len(labels)) + 1\n",
    "\n",
    "frame_assignments = []\n",
    "for label in labels:\n",
    "    frame_assignments.extend([label] * samples_per_step)\n",
    "frame_assignments = np.array(frame_assignments)\n",
    "\n",
    "sections = frame_assignments == cluster_idx\n",
    "\n",
    "song_section = song[sections[:len(song)]]\n",
    "\n",
    "IPython.display.Audio(song_section, rate=sr, autoplay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# create musicnn prompts\n",
    "clip_prompts = []\n",
    "pbar = tqdm(list(used_tag_df.iterrows()))\n",
    "\n",
    "for i, row in pbar:\n",
    "    row = row[row > tag_df_means]\n",
    "    sorted_row = row.sort_values(ascending=False)\n",
    "\n",
    "    # generate clip prompt for current musicnn targets\n",
    "    if prompt_mode == \"top_k\":\n",
    "        # get tags\n",
    "        top_tag_names = list(sorted_row.iloc[:k].index)\n",
    "        #print(top_tag_names)\n",
    "        pbar.set_description(\", \".join(top_tag_names))\n",
    "        clip_prompt = \", \".join(top_tag_names)\n",
    "    elif prompt_mode == \"weighted_top_k\":\n",
    "        sorted_row = filter_theme_labels(sorted_row, n=k)#, factor=0.8, threshold=0.1)\n",
    "        top_tag_names = list(sorted_row.index)\n",
    "        top_tag_vals = list(sorted_row)\n",
    "        clip_prompt = {name: val for name, val in zip(top_tag_names, top_tag_vals)}\n",
    "    elif prompt_mode == \"gpt\":\n",
    "        sorted_row = row.sort_values(ascending=False)\n",
    "        top_tags = sorted_row.iloc[:k]\n",
    "        top_tag_names = list(top_tags.index)\n",
    "        if len(top_tag_names) == 0:\n",
    "            top_tag_names = [\"Undecided emptiness\"]\n",
    "        merged_top_tags = \", \".join(top_tag_names)\n",
    "        if merged_top_tags in prompt_hash_table:\n",
    "            clip_prompt = prompt_hash_table[merged_top_tags]\n",
    "        else:\n",
    "            clip_prompt = gpt_create_prompt(gpt_model, gpt_tokenizer, merged_top_tags)\n",
    "            pbar.set_description(\"Tags: \" + merged_top_tags + \" Prompt: \" + clip_prompt)\n",
    "            #clip_encoding = imagine.create_text_encoding(clip_prompt)\n",
    "            prompt_hash_table[merged_top_tags] = clip_prompt\n",
    "            \n",
    "    clip_prompts.append(clip_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many steps are there to fill at the start of the song (256 is the size of the fft-windows of musicnn)\n",
    "start_prompt = clip_prompts[0]\n",
    "n_start_prompts = int(np.round((len(song) / (256 * averaging_window) - len(used_tag_df))))\n",
    "clip_prompts.extend([start_prompt] * n_start_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../StyleCLIP_modular\")\n",
    "sys.path.append(\"../CLIPGuidance\")\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from style_clip import Imagine, create_text_path\n",
    "\n",
    "net = \"vqgan\" # conv, vqgan\n",
    "\n",
    "args = {}\n",
    "args[\"lr_schedule\"] = 0\n",
    "args[\"seed\"] = 1\n",
    "\n",
    "args[\"neg_text\"] = None #'incoherent, confusing, cropped, watermarks'\n",
    "#'text, signature, watermarks, writings, scribblings'#\n",
    "\n",
    "args[\"clip_names\"] = [\"ViT-B/16\", \"ViT-B/32\"]#, \"RN50\"]\n",
    "args[\"averaging_weight\"] = 0\n",
    "args[\"early_stopping_steps\"] = 0\n",
    "args[\"use_tv_loss\"] = 1\n",
    "args[\"lpips_weight\"] = lpips_weight\n",
    "\n",
    "if net == \"vqgan\":\n",
    "    args[\"model_type\"] = \"vqgan\"\n",
    "    args[\"lr\"] = 0.1\n",
    "    \n",
    "elif net == \"conv\":\n",
    "    args[\"model_type\"] = \"conv\"\n",
    "    args[\"act_func\"] = \"gelu\"\n",
    "    args[\"stride\"] = 1\n",
    "    args[\"num_layers\"] = 5\n",
    "    args[\"downsample\"] = False\n",
    "    args[\"norm_type\"] = \"layer\"\n",
    "    args[\"num_channels\"] = 64\n",
    "    args[\"sideX\"] = 1080\n",
    "    args[\"sideY\"] = 720\n",
    "    args[\"lr\"] = 0.005\n",
    "    args[\"stack_size\"] = 4\n",
    "\n",
    "\n",
    "args[\"batch_size\"] = clip_batch_size\n",
    "args[\"sideX\"] = sideX # 688 #624 #544 #480 \n",
    "args[\"sideY\"] = sideY # 384 #352 #304 #272 \n",
    "# 688x384 - 7.792GB, 34s/it\n",
    "# 720x400 - 7.948GB, 41.3s/it - crashes after a bit\n",
    "# 624x352 - 6.9GB, 29.8s/it\n",
    "# 544x304 - 5850MB, 24s/it at 100its per step\n",
    "args[\"circular\"] = 0\n",
    "\n",
    "imagine = Imagine(\n",
    "                save_progress=False,\n",
    "                open_folder=False,\n",
    "                save_video=False,\n",
    "                verbose=False,\n",
    "                **args\n",
    "               )\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip_model = imagine.perceptor.models[0]\n",
    "#gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = \"The following are adjectives describing a song, followed by a description of the corresponding image:\\n \"\n",
    "#prefix = \"The following are adjectives describing an image, listed in the order of importance. They are followed by a full description of the corresponding image:\\n \"\n",
    "#prompter = \". Full description:\"\n",
    "#examples = {\"sad, dark, fast\": \" A man is running through dark woods while crying.\",\n",
    "#            \"sad, beautiful, soft, quiet, slow\": \" An old woman is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "#            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors forming strong geometric shapes that resemble a rave.\",\n",
    "#            \"weird, happy, fast\": \" A man is experiencing a strange dream. He is struggling to feel his feelings, his emotions as they rush too quickly through his body. He is an a state of ecstacy.\",\n",
    "#            \"harmonious, mellow\": \" An electric light begins to dim at a distant point in the sky. You feel complete and at one with your environment.\",\n",
    "#            \"slow, quiet\": \" A lion's roar stops in front of him. The lion is slowly moving forward and approaching you. He is silent.\"\n",
    "#           }\n",
    "\n",
    "#target_text = \"slow, brutal\"\n",
    "#target_clip_feats = clip_model.encode_text(tokenize(target_text).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"An ancient dream starts to calm down. It is quiet and peaceful\"\n",
    "\"An ordinary man appears before you. He is listening to you. He is a crazed crazy crazy crazy crazy crazy. He is beating on your legs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "#             start_text=\"\", p=0.9, \n",
    " ##            prefix=prefix, examples=examples, prompter=prompter, target_text=target_text,\n",
    " #            clip_weight=0.9, \n",
    " #            clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=5)\n",
    "#print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from mustovi_utils import load_gpt_model, gen_sent\n",
    "from clip import tokenize\n",
    "\n",
    "\n",
    "def gpt_create_prompt(cluster_words_list, gpt_name, clip_model, gpt_model=None, gpt_tokenizer=None):\n",
    "    if gpt_model is None:\n",
    "        gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)\n",
    "\n",
    "    prefix = \"The following are adjectives describing a song, followed by a description of the corresponding image:\\n \"\n",
    "    prefix = \"The following are adjectives describing an image, listed in the order of importance. They are followed by a full description of the corresponding image:\\n \"\n",
    "    prompter = \". Full description:\"\n",
    "    #examples = {\"sad, dark, fast\": \" A man is running through dark woods while crying.\",\n",
    "    #            \"sad, beautiful, soft, quiet, slow\": \" An old woman is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "    #            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors forming strong geometric shapes that resemble a rave.\",\n",
    "    #           }\n",
    "    examples = {\"sad, dark, fast\": \" A man is running through dark woods while crying.\",\n",
    "            \"sad, beautiful, soft, quiet, slow\": \" An old woman is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors forming strong geometric shapes that resemble a rave.\",\n",
    "            \"weird, happy, fast\": \" A man is experiencing a strange dream. He is struggling to feel his feelings, his emotions as they rush too quickly through his body. He is an a state of ecstacy.\",\n",
    "            \"harmonious, mellow\": \" An electric light begins to dim at a distant point in the sky. You feel complete and at one with your environment.\",\n",
    "            \"slow, quiet\": \" A lion's roar stops in front of him. The lion is slowly moving forward and approaching you. He is silent.\"\n",
    "           }\n",
    "\n",
    "    \n",
    "    gpt_stories = []\n",
    "    for target_text in tqdm(cluster_words_list):\n",
    "        target_clip_feats = clip_model.encode_text(tokenize(target_text).to(\"cuda\"))\n",
    "\n",
    "        texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "                 start_text=\"\", p=0.9, \n",
    "                 prefix=prefix, examples=examples, prompter=prompter, target_text=target_text,\n",
    "                 clip_weight=0.4, \n",
    "                 clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=1)\n",
    "        text = texts[0]\n",
    "        gpt_stories.append(text)\n",
    "        print(text)\n",
    "        print()\n",
    "    gpt_model = gpt_model.to(\"cpu\")\n",
    "    return gpt_stories\n",
    "\n",
    "used_gpt_stories = None\n",
    "if do_create_gpt_cluster_stories:\n",
    "    used_gpt_stories = gpt_create_prompt(cluster_themes, gpt_name, imagine.perceptor.models[0])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_gpt_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mustovi_utils import load_gpt_model, gen_sent\n",
    "from clip import tokenize\n",
    "def gpt_create_theme(theme_words, gpt_name, clip_model, gpt_model=None, gpt_tokenizer=None):\n",
    "    if gpt_model is None:\n",
    "        gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)\n",
    "\n",
    "    prefix = \"The following are adjectives, followed by a matching artstyle:\\n \"\n",
    "    prompter = \". Matching artstyle:\"\n",
    "    \n",
    "    prompter = \"The name of a matching painter is:\"\n",
    "    prefix= \"The following are lists of words describing art, followed by the name of the artist:\\n \"\n",
    "    \n",
    "    prefix = \"The following are lists of words describing art, followed by the name of the artist:\\n \"\n",
    "    prompter = \"Matching visual artist:\"\n",
    "    \n",
    "    prefix = \"The following are lists of adjectives, listed in order of importance. They are followed by a name of an artstyle that matches them:\\n \"\n",
    "    prompter = \". Matching artstyle:\"\n",
    "        \n",
    "    examples = {\"introspective, beautiful, sad\": \" A moody painting.\",\n",
    "                \"expressive, wild, colourful\": \" An expressionist piece of art.\",\n",
    "                \"epic, fantasy, stunning, moody\": \" Illustrated by Greg Rutkowski.\",\n",
    "                \"realistic, beautiful, landscapes, forgotten civilizations\": \" By James Gurney.\"}\n",
    "    # introspective, beautiful, sad{prompter} Impressionism.\n",
    "    # popular, internet{prompter} Trending on artstation.\n",
    "    # rendered, detailed, high-quality{prompter} Rendered in unreal engine.\n",
    "    # expressionist, beautiful, vibrant. {prompter} Van Gogh.\n",
    "    # happy, dreamy, romantic, sensual. {prompter} Gustav Klimt.\n",
    "    \n",
    "    target_clip_feats = clip_model.encode_text(tokenize(theme_words).to(\"cuda\"))\n",
    "\n",
    "    texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "                 start_text=\"\", p=0.9, \n",
    "                 prefix=prefix, examples=examples, prompter=prompter, target_text=theme_words,\n",
    "                 clip_weight=0.4, \n",
    "                 clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=2)\n",
    "    text = texts[0]\n",
    "    gpt_model = gpt_model.to(\"cpu\")\n",
    "    return text\n",
    "\n",
    "if create_gpt_artstyle:\n",
    "    gpt_theme = gpt_create_theme(main_theme_words, gpt_name, imagine.perceptor.models[0])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gpt_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clip_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate encodings based on prompts\n",
    "\n",
    "clip_target_encodings = []\n",
    "clip_feature_hash_table = dict()\n",
    "gpt_suffix = \"\" if len(gpt_theme) == 0 else f\" {gpt_theme}\"\n",
    "\n",
    "count = []\n",
    "\n",
    "def encode(prompt):\n",
    "    prompt = prefix + prompt\n",
    "    if general_theme is not None:\n",
    "        prompt = prompt + general_theme\n",
    "    prompt += gpt_suffix\n",
    "    if prompt in clip_feature_hash_table:\n",
    "        encoding = clip_feature_hash_table[prompt]\n",
    "    else:\n",
    "        count.append(0)\n",
    "        if len(count) % 50 == 0:\n",
    "            print(prompt)\n",
    "        encoding = imagine.create_clip_encoding(text=prompt, img=img_theme)\n",
    "        #encoding = imagine.create_text_encoding(prompt)\n",
    "        clip_feature_hash_table[prompt] = encoding\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def weighted_average_encoding(encodings, weights):\n",
    "    clip_encoding = [torch.stack([norm(encoding[j]) * weight for encoding, weight in zip(encodings, weights)]).sum(dim=0)\n",
    "                         for j in range(len(encodings[0]))]\n",
    "    weight_sum = sum(weights)\n",
    "    clip_encoding = [norm(enc / weight_sum) for enc in clip_encoding]\n",
    "    return clip_encoding\n",
    "\n",
    "\n",
    "def norm(a):\n",
    "    return a / a.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def clip_mean_direction(direction_prompt, base_prompts, imagine):\n",
    "    base_encs = [imagine.create_clip_encoding(text=p) for p in base_prompts]\n",
    "    base_plus_dir_encs = [imagine.create_clip_encoding(text=p + direction_prompt) for p in base_prompts]\n",
    "    diff_encs = [[norm(norm(base_ext_enc[i]) - norm(base_enc[i])) for i in range(len(base_enc))] for base_enc, base_ext_enc in zip(base_encs, base_plus_dir_encs)]\n",
    "    mean_diff_enc = [norm(torch.stack([diff_encs[j][i] for j in range(len(diff_encs))]).mean(dim=0)) for i in range(len(diff_encs[0]))]\n",
    "    return mean_diff_enc\n",
    "\n",
    "\n",
    "\n",
    "if use_mean_dirs:\n",
    "    base_prompts = [\"A photo of \", \" \", \"A painting of \", \"This painting is: \", \"This photo looks \", \"I feel \", \"I feel: \", \"The sky is \",\n",
    "                   \"This is \", \"The ground is \", \"This person is \", \"She is \", \"He  is \", \"A \"]\n",
    "    dir_dict = {col: clip_mean_direction(col, base_prompts, imagine) for col in used_tag_df}\n",
    "\n",
    "\n",
    "for idx, prompt in enumerate(tqdm(clip_prompts)):\n",
    "    gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                             dist_to_centers, gpt_story_top_k, idx)\n",
    "    \n",
    "    story_encodings = []\n",
    "    for gpt_story, story_weight in zip(gpt_stories, story_weights):\n",
    "        if isinstance(prompt, dict):\n",
    "            if use_mean_dirs:\n",
    "                take_avg = True\n",
    "                \n",
    "                \n",
    "                clip_encoding = encode(gpt_story + \" \")\n",
    "                \n",
    "                if take_avg:\n",
    "                    dir_encodings = [dir_dict[tag] for tag in prompt]\n",
    "                    weights = list(prompt.values())\n",
    "                    if len(dir_encodings) > 0:\n",
    "                        dir_mean_encoding = weighted_average_encoding(dir_encodings, weights)\n",
    "                        clip_encoding = weighted_average_encoding([clip_encoding, dir_mean_encoding], [1 - avg_weight, avg_weight])\n",
    "                else:\n",
    "                    for tag in prompt:\n",
    "                        weight = prompt[tag]\n",
    "                        clip_encoding = [clip_encoding[i] + dir_dict[tag][i] * weight for i in range(len(clip_encoding))]\n",
    "                clip_encoding = [norm(enc) for enc in clip_encoding]\n",
    "            else:\n",
    "                encodings = [encode(gpt_story + \" It feels \" + prompt_key + \".\") for prompt_key in prompt]\n",
    "                weights = list(prompt.values())\n",
    "                clip_encoding = weighted_average_encoding(encodings, weights)\n",
    "        else:\n",
    "            story_prompt = gpt_story + \" \" + prompt + \".\"\n",
    "            clip_encoding = encode(story_prompt)\n",
    "        story_encodings.append(clip_encoding)\n",
    "    clip_encoding = weighted_average_encoding(story_encodings, story_weights)\n",
    "\n",
    "    \n",
    "    clip_encoding = [enc.to(\"cpu\") for enc in clip_encoding]\n",
    "    clip_target_encodings.append(clip_encoding)\n",
    "    \n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[430]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_target_encodings[10][0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ema of encodings to smoothen\n",
    "ema_encodings = []\n",
    "ema = clip_target_encodings[0]\n",
    "\n",
    "for encoding in clip_target_encodings:\n",
    "    ema = [ema_val * ema[i].to(\"cpu\") + (1 - ema_val) * encoding[i].to(\"cpu\") for i in range(len(encoding))]\n",
    "    ema_encodings.append(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "from mustovi_utils import get_spec_norm\n",
    "import librosa\n",
    "# create zoom, rotate, shift effects\n",
    "effects = [\"zoom\", \"rotate\", \"shiftX\", \"shiftY\", \"shear\"]\n",
    "harm_effect_dict = {\"rotate\": 0.0}\n",
    "perc_effect_dict = {\"zoom\": -0.5}\n",
    "cqt_effect_dict = [{\"zoom\": 1.0}, \n",
    "                   {\"rotate\": 1.0},\n",
    "                   {\"shiftX\": 1.0}, \n",
    "                   {\"shiftY\": 1.0},\n",
    "                   {\"shiftY\": -1.0},\n",
    "                   {\"shiftX\": -1.0},\n",
    "                   {\"rotate\": -1.0},\n",
    "                   {\"zoom\": -1.0},\n",
    "                  ]\n",
    "# divide song in percussion and harm (might divide in pitches later)\n",
    "song_harm, song_perc = librosa.effects.hpss(song)\n",
    "spec_norm_harm = get_spec_norm(song_harm)\n",
    "spec_norm_perc = get_spec_norm(song_perc)\n",
    "# get cqt spec\n",
    "n_chroma = len(cqt_effect_dict)\n",
    "cqt_spec = librosa.feature.chroma_cqt(y=song, sr=sr,hop_length=256, \n",
    "                                      n_chroma=n_chroma, n_octaves=7, \n",
    "                                      bins_per_octave=n_chroma * 4, norm=None)\n",
    "sns.heatmap(cqt_spec)\n",
    "plt.show()\n",
    "# take window averages to match video fps\n",
    "N = averaging_window\n",
    "spec_norm_harm = np.convolve(spec_norm_harm, np.ones(N) / N , mode='valid')[::N]\n",
    "spec_norm_perc = np.convolve(spec_norm_perc, np.ones(N) /N, mode='valid')[::N]\n",
    "cqt_spec = np.array([np.convolve(cqt_line, np.ones(N) / N, mode='valid')[::N] \n",
    "                     for cqt_line in cqt_spec])\n",
    "# min-max norm\n",
    "spec_norm_harm = (spec_norm_harm - spec_norm_harm.min()) / (spec_norm_harm.max() - spec_norm_harm.min())\n",
    "spec_norm_perc = (spec_norm_perc - spec_norm_perc.min()) / (spec_norm_perc.max() - spec_norm_perc.min())\n",
    "cqt_spec = (cqt_spec - cqt_spec.min()) / (cqt_spec.max() - cqt_spec.min())\n",
    "# create effects\n",
    "class Effect:\n",
    "    def __init__(self, strength, zoom=0, rotate=0, \n",
    "                 shiftX=0, shiftY=0, shear=0):\n",
    "        max_zoom = 0.22\n",
    "        self.zoom = 1 + max_zoom * zoom * strength\n",
    "        max_rotate = 10\n",
    "        self.rotate = max_rotate * rotate * strength\n",
    "        max_shift = 12\n",
    "        self.shift_x = max_shift * shiftX * strength\n",
    "        self.shift_y = max_shift * shiftY * strength\n",
    "        \n",
    "        self.shear = 0\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        # transform it\n",
    "        transformed_img = T.functional.affine(img, \n",
    "                                          angle=self.rotate, \n",
    "                                          translate=(self.shift_x,\n",
    "                                                     self.shift_y), \n",
    "                                          scale=self.zoom, \n",
    "                                          shear=self.shear,\n",
    "                                          fill=0,\n",
    "                                          interpolation=torchvision.transforms.InterpolationMode.BILINEAR\n",
    "                                         )\n",
    "        # fill in zeros with nearest neighbor\n",
    "        data = transformed_img.numpy()\n",
    "        mask = np.where(~(data == 0))\n",
    "        interp = NearestNDInterpolator(np.transpose(mask), data[mask])\n",
    "        image_result = interp(*np.indices(data.shape))\n",
    "        return torch.from_numpy(image_result)\n",
    "        \n",
    "\n",
    "def merge_dicts(effect_dict, effect_strength_dict, amplitude):\n",
    "    for key in effect_strength_dict:\n",
    "        content = effect_strength_dict[key] * amplitude\n",
    "        if key in effect_dict:\n",
    "            effect_dict[key] += content\n",
    "        else:\n",
    "            effect_dict[key] = content\n",
    "\n",
    "\n",
    "effects_list = []\n",
    "for i in range(len(spec_norm_harm)):\n",
    "    harm = spec_norm_harm[i]\n",
    "    perc = spec_norm_perc[i]\n",
    "    cqt = cqt_spec[:, i]\n",
    "    \n",
    "    effect_dict = {}\n",
    "    merge_dicts(effect_dict, harm_effect_dict, harm)\n",
    "    merge_dicts(effect_dict, perc_effect_dict, perc)\n",
    "    for cqt_effect, cqt_amplitude in zip(cqt_effect_dict, cqt):\n",
    "        merge_dicts(effect_dict, cqt_effect, cqt_amplitude)\n",
    "    \n",
    "    effect = Effect(total_effect_strength, **effect_dict)\n",
    "    effects_list.append([effect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ema_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(effects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def sequential_gen(ema_encodings, effects_list):\n",
    "    if len(ema_encodings) > len(effects_list):\n",
    "        ema_encodings = ema_encodings[:-1]\n",
    "    assert len(ema_encodings) == len(effects_list), f\"{len(ema_encodings)}, {len(effects_list)}\"\n",
    "\n",
    "\n",
    "    img_latents = []\n",
    "    img = None\n",
    "    imagine.to(\"cuda\")\n",
    "    imagine.reset()\n",
    "    transformed_img = None\n",
    "\n",
    "    pbar = tqdm(list(range(len(ema_encodings))))\n",
    "    for i in pbar:\n",
    "        clip_encoding, effects = ema_encodings[i], effects_list[i]\n",
    "        # apply effects\n",
    "        if img is not None:\n",
    "            transformed_img = img.float()\n",
    "            for effect in effects:\n",
    "                transformed_img = effect(transformed_img)\n",
    "            transformed_img = transformed_img.mul(2).sub(1).to(imagine.device)\n",
    "            latent, _, [_, _, indices] = imagine.model.model.model.encode(transformed_img)\n",
    "            imagine.set_latent(latent)\n",
    "        # set target encoding in CLIP\n",
    "        clip_encoding = [part.to(imagine.device) for part in clip_encoding]\n",
    "        imagine.set_clip_encoding(encoding=clip_encoding)\n",
    "        # optimize for some steps\n",
    "        for _ in range(sub_steps):\n",
    "            img, loss = imagine.train_step(0, 0, lpips_img=transformed_img)\n",
    "        img = img.detach().cpu()\n",
    "        # get latent of img\n",
    "        latent = imagine.model.model.get_latent().detach().cpu()\n",
    "        img_latents.append(latent)\n",
    "        # save final img\n",
    "        if i % (len(ema_encodings) // 20) == 0:\n",
    "            pil_img = to_pil(img.squeeze())\n",
    "\n",
    "            display(pil_img)\n",
    "            #clear_output(wait = True)\n",
    "    return img_latents\n",
    "\n",
    "\n",
    "def unflatten_encodings(encodings, enc_idcs):\n",
    "    return [unflatten_encoding(enc, enc_idcs) for enc in encodings]\n",
    "\n",
    "\n",
    "def unflatten_encoding(encoding, enc_idcs):\n",
    "    return [encoding[idcs[0]:idcs[1]] for idcs in enc_idcs]\n",
    "\n",
    "\n",
    "def get_gpt_stories_and_weights(cluster_gpt_stories, n_start_prompts, dist_to_centers, gpt_story_top_k, idx, t=50):\n",
    "    if cluster_gpt_stories is not None:\n",
    "        story_idx = max(idx - n_start_prompts, 0)\n",
    "        dists = dist_to_centers[story_idx]\n",
    "        dists = 1 - (dists / dists.max())\n",
    "        dists = torch.nn.functional.softmax(dists * t, dim=-1)\n",
    "        top_k = dists.topk(k=gpt_story_top_k, largest=True)\n",
    "        #story_weights = (1 - (top_k.values / dist_to_centers[story_idx].max())) ** 10\n",
    "        story_weights = top_k.values\n",
    "        top_idcs = top_k.indices\n",
    "        gpt_stories = [cluster_gpt_stories[i] for i in top_idcs]\n",
    "    else:\n",
    "        gpt_stories = [\"\"]\n",
    "        story_weights = [1]\n",
    "    return gpt_stories, story_weights\n",
    "\n",
    "\n",
    "def parallel_gen(clip_prompts):\n",
    "    sub_steps = 100\n",
    "    \n",
    "    all_encodings = []\n",
    "    for i, prompt in enumerate(clip_prompts):\n",
    "        gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                                 dist_to_centers, gpt_story_top_k, i)\n",
    "        story_prompt = gpt_stories[0]\n",
    "        encoding = encode(story_prompt)\n",
    "        all_encodings.append(encoding)\n",
    "        \n",
    "    latent_dict = dict()\n",
    "    \n",
    "    enc_lens = [len(enc) for enc in all_encodings[0]]\n",
    "    enc_idcs = []\n",
    "    last_idx = 0\n",
    "    for l in enc_lens:\n",
    "        enc_idcs.append((last_idx, last_idx + l))\n",
    "        last_idx += l\n",
    "    print(enc_lens)\n",
    "    print(enc_idcs)\n",
    "    print(torch.cat(all_encodings[0]).shape)\n",
    "    flat_targets = [torch.cat(enc).float() for enc in all_encodings]\n",
    "    \n",
    "    unique_targets = torch.unique(torch.stack(flat_targets), dim=0)\n",
    "    print(\"Num unique_targets: \", len(unique_targets))\n",
    "    assert len(unique_targets) < 100\n",
    "    \n",
    "    \n",
    "    target_dict = dict()\n",
    "    img_latents = []\n",
    "    for i, prompt in enumerate(tqdm(clip_prompts)):\n",
    "        gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                                 dist_to_centers, gpt_story_top_k, i, t=100)\n",
    "        story_prompt = gpt_stories[0]\n",
    "        for story_prompt in gpt_stories:\n",
    "            if story_prompt in target_dict:\n",
    "                latent = target_dict[story_prompt]\n",
    "            else:\n",
    "                print(story_prompt)\n",
    "                encoding = encode(story_prompt)\n",
    "    \n",
    "        #for target in flat_targets:\n",
    "         #   if target in latent_dict:\n",
    "         #       latent = latent_dict[target]\n",
    "         #   else:\n",
    "                imagine.reset()\n",
    "                #unflattend_target = unflatten_encoding(target.half(), enc_idcs)\n",
    "                imagine.set_clip_encoding(encoding=encoding)\n",
    "                for _ in range(sub_steps):\n",
    "                    img, loss = imagine.train_step(0, 0)\n",
    "                latent = imagine.model.model.get_latent().detach().cpu()\n",
    "                target_dict[story_prompt] = latent\n",
    "                pil_img = to_pil(img.squeeze())\n",
    "                display(pil_img)\n",
    "        \n",
    "        #if i % 10 == 0:\n",
    "        #    print(story_weights)\n",
    "        story_encodings = [target_dict[story_prompt] for story_prompt in gpt_stories]\n",
    "        clip_encoding = torch.sum(torch.stack([enc * weight for enc, weight in zip(story_encodings, story_weights)]), dim=0) / sum(story_weights)\n",
    "        img_latents.append(clip_encoding.clone())\n",
    "    \n",
    "    return img_latents\n",
    "\n",
    "img_latents = sequential_gen(ema_encodings, effects_list)\n",
    "#img_latents = parallel_gen(clip_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ema of encodings to smoothen\n",
    "ema_latents = apply_ema(img_latents, ema_val=ema_val_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate between latents to increase fps and make video smoother\n",
    "from mustovi_utils import slerp\n",
    "\n",
    "goal_frame_count = boost_fps * len(song) / 16000\n",
    "current_frame_count = len(ema_latents)\n",
    "frames_to_add = np.ceil(goal_frame_count / current_frame_count)\n",
    "if frames_to_add > 1:\n",
    "    video_latents = []\n",
    "    for i, latent in enumerate(ema_latents):\n",
    "        if i + 1 == len(ema_latents):\n",
    "            break\n",
    "        else:\n",
    "            next_latent = ema_latents[i + 1]\n",
    "        latents_to_add = [slerp(latent, next_latent, frac) \n",
    "                          for frac in np.arange(frames_to_add) / frames_to_add]\n",
    "        video_latents.extend(latents_to_add)\n",
    "else:\n",
    "    video_latents = ema_latents\n",
    "len(video_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.save(video_latents, \"video_latents.pt\")\n",
    "#video_latents = torch.load(\"video_latents.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "import soundfile\n",
    "import moviepy.editor as mpy\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_img_paths(root):\n",
    "    paths = [os.path.join(root, f) for f in os.listdir(root)\n",
    "        if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "    paths = sorted(paths, key= lambda x: int(x.split(\"/\")[-1].split(\"_\")[0].split(\".\")[0]))\n",
    "    return paths\n",
    "\n",
    "\n",
    "def create_video(imgs, song_name, vid_name, sr, bitrate=\"5000k\"):\n",
    "    os.makedirs(\"video_gens\", exist_ok=True)\n",
    "    video_path = f\"video_gens/{vid_name}\"\n",
    "    \n",
    "    if isinstance(imgs, list):\n",
    "        img_len = len(imgs)\n",
    "    else:\n",
    "        img_len = len([img for img in os.listdir(imgs) \n",
    "                       if img.endswith(\".jpg\") or img.endswith(\".png\")])\n",
    "        \n",
    "        \n",
    "    audio = mpy.AudioFileClip(song_name, fps=sr, nbytes=4)        \n",
    "        \n",
    "    vid_fps = len(imgs) / audio.duration\n",
    "\n",
    "    # Write temporary audio file\n",
    "    #soundfile.write('tmp.wav', song, sr)\n",
    "\n",
    "    # Generate final video\n",
    "    #audio = mpy.AudioFileClip(\"tmp.wav\", fps=sr)\n",
    "    #from moviepy.audio.AudioClip import AudioArrayClip\n",
    "    #audio = AudioArrayClip(np.expand_dims(song, axis=0), fps=sr) # from a numerical array\n",
    "    video = mpy.ImageSequenceClip(imgs, fps=vid_fps)\n",
    "    video = video.set_audio(audio)\n",
    "    \n",
    "    video.write_videofile(video_path, \n",
    "                      codec=\"libx264\",\n",
    "                      fps=vid_fps,\n",
    "                      #audio_codec=\"libmp3lame\",\n",
    "                      audio_codec=\"aac\",\n",
    "                      threads=10,\n",
    "                      bitrate=bitrate,\n",
    "                      audio_bitrate=\"256k\",\n",
    "                      preset=\"slow\",\n",
    "                     )\n",
    "    \n",
    "    # Delete temporary audio file\n",
    "    #os.remove('tmp.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create latents and save to disk:\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "extension = \".jpg\"\n",
    "tmp_folder = \"tmp/vid_imgs\" \n",
    "if os.path.exists(tmp_folder):\n",
    "    shutil.rmtree(tmp_folder)\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "\n",
    "gen_model = imagine.model.model\n",
    "imagine.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "gen_model.to(device)\n",
    "\n",
    "\n",
    "for i, latent in enumerate(tqdm(video_latents)):\n",
    "    img = gen_model(latents=latent.to(device)).to(\"cpu\")\n",
    "    pil_img = to_pil(img.squeeze())\n",
    "    pil_img.save(os.path.join(tmp_folder, f\"{i}{extension}\"), subsampling=0, quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%m_%d_%H:%M\")  #(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "video_name = f\"{song_name.split('/')[-1].split('.')[0]}_{prompt_mode}_ema{ema_val}_steps{sub_steps}\"\n",
    "video_name += \"_\"+ gpt_theme.replace(\" \", \"_\") if create_gpt_artstyle else \"\"\n",
    "video_name += f\"_{args['model_type']}_{date_time}.mp4\"\n",
    "\n",
    "paths = load_img_paths(\"tmp/vid_imgs\")\n",
    "create_video(paths, song_name, video_name, old_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
    "if not os.path.exists(\"Real-ESRGAN\"):\n",
    "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "    %cd Real-ESRGAN\n",
    "    # Set up the environment\n",
    "    !$PYTHONPATH -m pip install basicsr facexlib gfpgan\n",
    "    !$PYTHONPATH -m pip install -r requirements.txt\n",
    "    !$PYTHONPATH setup.py develop\n",
    "    # Download the pre-trained model\n",
    "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
    "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth -P experiments/pretrained_models\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Real-ESRGAN\")\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs(imgs, out_folder=None, scale=4, tile=0):\n",
    "    \n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    upsampler = RealESRGANer(\n",
    "        scale=scale,\n",
    "        model_path=\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\",\n",
    "        model=model,\n",
    "        tile=tile,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=1)\n",
    "    \n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            img = np.array(Image.open(img))[:,:,::-1]\n",
    "        \n",
    "        output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs\n",
    "\n",
    "\n",
    "import torchvision\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs_custom(imgs, out_folder=None, scale=4, tile=0):\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    loadnet = torch.load(\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\")\n",
    "    if 'params_ema' in loadnet:\n",
    "        keyname = 'params_ema'\n",
    "    else:\n",
    "        keyname = 'params'\n",
    "    model.load_state_dict(loadnet[keyname], strict=True)\n",
    "    model.eval().cuda().half()\n",
    "\n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            #img = torch.from_numpy(np.ascontiguousarray(Image.open(img))[:,:,::-1].copy()).unsqueeze(0)\n",
    "            img = torch.from_numpy(np.transpose(np.array(Image.open(img))[:,:,::-1].copy(), (2, 0, 1))).float().unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        output = model(img.half()).cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        #output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        output = (output * 255.0).round().astype(np.uint8)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def load_images_from_mp4(path):\n",
    "    vid = imageio.get_reader(path,  'ffmpeg')\n",
    "    imgs = [np.array(image) for image in vid.iter_data()]\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "upscale = True\n",
    "\n",
    "if upscale:\n",
    "    input_folder = \"tmp/vid_imgs\"\n",
    "    output_folder = \"tmp/upscaled_vid_imgs\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"Upscaling...\")\n",
    "    \n",
    "    #video_path = \"video_gens/any_colour_you_like_pink_floyd_hd_studio_quality_7032261705832661515_top_k_ema0.2_steps100Weird_and_beautiful._vqgan_11_20_14:02.mp4\"\n",
    "    #video_name = video_path.split(\"/\")[0]\n",
    "    #input_paths = load_images_from_mp4(video_name)\n",
    "    \n",
    "    input_paths = load_img_paths(input_folder)\n",
    "    upscale_imgs(input_paths, out_folder=output_folder, scale=4)\n",
    "    #!CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth --input $input_folder --output $output_folder  --netscale 4 --outscale 4 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    upscaled_video_name = video_name.split(\"/\")\n",
    "    upscaled_video_name[-1] = \"HD_\" + upscaled_video_name[-1]\n",
    "    upscaled_video_name = \"/\".join(upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)\n",
    "    create_video(paths, song_name, upscaled_video_name, old_sr, bitrate=\"12000k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "if twice_upscale:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    input_folder = \"tmp/upscaled_vid_imgs\"\n",
    "    output_folder = \"tmp/twice_upscaled_vid_imgs\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)    \n",
    "    print(\"Upscaling...\")\n",
    "    input_paths = load_img_paths(input_folder)\n",
    "    upscale_imgs(input_paths, out_folder=output_folder, scale=4)\n",
    "    #!CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x2plus.pth --input  $input_folder --output $output_folder  --netscale 2 --outscale 2 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    twice_upscaled_video_name = upscaled_video_name.split(\"/\")\n",
    "    twice_upscaled_video_name[-1] = \"Full\" + twice_upscaled_video_name[-1]\n",
    "    twice_upscaled_video_name = \"/\".join(twice_upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)\n",
    "    create_video(paths, song_name, twice_upscaled_video_name, old_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

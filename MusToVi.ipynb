{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418b6ff-b263-4772-bd3e-2860787348f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arg_utils import is_notebook, get_cfg\n",
    "cfg = get_cfg()\n",
    "# override variables to experiment in notebook\n",
    "if is_notebook():\n",
    "    cfg[\"gpu\"] = 0\n",
    "    cfg[\"song_name\"] = \"songs/gnossi_1.mp3\"\n",
    "\n",
    "locals().update(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b332e-6449-47cc-aaed-b58137aadeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder where to store temporary calculations (need quite some GBs)\n",
    "base_folder = \"/raid/8wiehe/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# params that need to be set for each computing node before importing torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "import torch\n",
    "gpu_avail = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-greenhouse",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "PYTHONPATH = sys.executable\n",
    "\n",
    "# set some parameters based on how much memory is available\n",
    "sideX, sideY = 656, 368\n",
    "clip_batch_size = 4\n",
    "gpt_name = \"neo1.3\"\n",
    "if gpu_avail >= 10.5:\n",
    "    clip_batch_size = 4\n",
    "    gpt_name = \"gptj\" #\"neo2.7\"\n",
    "    sideX, sideY = 880 - 16 * 2, 490 - 16\n",
    "elif gpu_avail >= 7.5:\n",
    "    clip_batch_size = 4\n",
    "    gpt_name = \"neo1.3\" # neo2.7 possible if it is also int8 quantized\n",
    "    # 7.6 GB for prompting, 8Gb for clip generation with (880, 490) and bs 8\n",
    "\n",
    "    \n",
    "if net == \"image\":\n",
    "    if hq:\n",
    "        sideX, sideY = 1920, 1080\n",
    "    else:\n",
    "        sideX, sideY = 1280, 720\n",
    "    upscale = False\n",
    "    clip_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-afghanistan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "resampled_path = \"tmp/resampled.wav\"\n",
    "os.makedirs(\"tmp\", exist_ok=True)\n",
    "\n",
    "# load song and resample to 16k Hz\n",
    "sr = 16000\n",
    "raw_song, old_sr = librosa.load(song_name, offset=offset, duration=duration)\n",
    "song = librosa.resample(raw_song, old_sr, sr)\n",
    "soundfile.write(resampled_path, song, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mustovi_utils import get_taggram\n",
    "    \n",
    "tag_dfs_folder = \"./tmp/tag_dfs\"\n",
    "os.makedirs(tag_dfs_folder, exist_ok=True)\n",
    "key_song_name = song_name.split(\"/\")[-1].split(\".\")[0]\n",
    "tag_df_name = f\"{key_song_name}_{input_length}_{int(1 / input_overlap)}_{offset}_{duration}.csv\"\n",
    "tag_df_path = os.path.join(tag_dfs_folder, tag_df_name)\n",
    "if os.path.exists(tag_df_path):\n",
    "    normed_tag_df = pd.read_csv(tag_df_path, index_col=0)\n",
    "else:\n",
    "    normed_tag_df = get_taggram(resampled_path, input_overlap, input_length)\n",
    "    normed_tag_df.to_csv(tag_df_path)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# ordered by importance\n",
    "show_all_labels = False\n",
    "if show_all_labels:\n",
    "    plt.figure(figsize=(12, 20))\n",
    "    show_df = normed_tag_df.T.copy()\n",
    "    show_df[\"mean\"] = normed_tag_df.mean(axis=0)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = [\"violin\", \"strings\", \"sitar\", \"piano\", \"harpsichord\", \n",
    "               \"harp\", \"guitar\", \"drums\", \"flute\", \"synth\", \"cello\"]\n",
    "genres = [\"techno\", \"soul\", \"rock\", \"rnb\", \"punk\", \"pop\", \"opera\", \n",
    "          \"oldies\", \"new age\", \"metal\", \"jazz\", \"indie rock\", \"indie pop\",\n",
    "          \"indie\", \"indian\", \"heavy metal\", \"hard rock\", \"funk\", \"folk\", \n",
    "          \"electronica\", \"electronic\", \"country\", \"classical\", \"classic rock\",\n",
    "          \"classic\", \"choral\", \"blues\", \"alternative rock\", \"alternative\",\n",
    "          \"Progressive rock\", \"House\", \"Hip-hop\"]\n",
    "eras = [\"60s\", \"70s\", \"80s\", \"90s\", \"00s\"]\n",
    "\n",
    "speed_tags = [\"fast\", \"slow\"]\n",
    "feeling_tags = [\"weird\", \"soft\", \"happy\", \"sad\", \"catchy\", \"easy listening\", \"sexy\", \"chillout\", \"beautiful\", \"chill\"]\n",
    "loudness_tags = [\"quiet\", \"loud\"]\n",
    "vibe_tags = [\"ambient\", \"party\", \"dance\", \"Mellow\", \"experimental\"]\n",
    "\n",
    "genre_like_tags = [\"solo\", \"blues\", \"Beat\"]\n",
    "\n",
    "feeling_tags = speed_tags + feeling_tags + loudness_tags + vibe_tags\n",
    "    \n",
    "plt.figure(figsize=(10, 7))\n",
    "show_df = normed_tag_df[feeling_tags].T.copy()\n",
    "show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "sns.heatmap(show_df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce taggram to fit fps\n",
    "musicnn_fps = 62.5\n",
    "#averaging_window = int(musicnn_fps / fps) # == 2 - 30fps\n",
    "averaging_window = int(np.round(musicnn_fps / fps)) # == 3 - 20fps\n",
    "# take step average taggram\n",
    "fps_taggram = normed_tag_df.rolling(averaging_window, min_periods=1, axis=0).mean() \n",
    "fps_taggram = fps_taggram.iloc[::averaging_window, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on using only subset\n",
    "used_tag_df = fps_taggram.copy()\n",
    "if taggram_mode == \"feelings\":    \n",
    "    used_tag_df = used_tag_df[feeling_tags]\n",
    "    \n",
    "# merge some columns\n",
    "merge_dict = {\"chill\": \"chillout\"}\n",
    "for key in merge_dict:\n",
    "    val = merge_dict[key]\n",
    "    used_tag_df[val] = (used_tag_df[key] + used_tag_df[val]) / 2\n",
    "    del used_tag_df[key]\n",
    "# rename some columns\n",
    "rename_dict = {\"sexy\": \"sensual\",\n",
    "               \"party\": \"energetic\",\n",
    "               \"dance\": \"moving\",\n",
    "               \"easy listening\": \"harmonious\",\n",
    "               \"catchy\": \"captivating\",\n",
    "               \"chillout\": \"relaxed\"}\n",
    "used_tag_df = used_tag_df.rename(columns=rename_dict)\n",
    "\n",
    "tag_df_means = used_tag_df.mean()\n",
    "used_tag_df[used_tag_df < tag_df_means] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ema(arr, ema_val=0.9):\n",
    "    ema = arr[0]\n",
    "    out = []\n",
    "    for item in arr:\n",
    "        ema = ema * ema_val + item * (1 - ema_val)\n",
    "        out.append(ema)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create clusters\n",
    "if create_clusters:\n",
    "    import sklearn\n",
    "    \n",
    "    clustering_feats = used_tag_df.to_numpy()\n",
    "    # add index to give some time continuity\n",
    "    cluster_time = True\n",
    "    if cluster_time:\n",
    "        idx_arr = np.expand_dims(np.arange(len(clustering_feats)), 1)\n",
    "        idx_arr = idx_arr / idx_arr.sum() * used_tag_df.sum().max() * cluster_time_weight\n",
    "        clustering_feats = np.concatenate([clustering_feats, idx_arr], axis=1)\n",
    "    # create high dim umap embeddings for clustering\n",
    "    cluster_on_umap_high_d = False\n",
    "    clusterable_embedding = np.array(apply_ema(clustering_feats, ema_val=ema_val_clustering))\n",
    "    \n",
    "    if use_k_means:\n",
    "        # cluster        \n",
    "        from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "        # Instantiate the clustering model and visualizer\n",
    "        model = clusterer = sklearn.cluster.KMeans(n_clusters=5, n_init=20, max_iter=500)\n",
    "        visualizer = KElbowVisualizer(\n",
    "            model, k=(min_clusters, max_clusters), metric='calinski_harabasz', #'silhouette', #'calinski_harabasz', \n",
    "            timings=False\n",
    "        )\n",
    "\n",
    "        visualizer.fit(clusterable_embedding)        # Fit the data to the visualizer\n",
    "        visualizer.show()  \n",
    "        num_clusters = visualizer.elbow_value_\n",
    "        # make final clustering\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=num_clusters, n_init=50, max_iter=500)\n",
    "        labels = clusterer.fit_predict(clusterable_embedding)\n",
    "        # determine centers\n",
    "        real_centers = clusterer.cluster_centers_\n",
    "        dist_to_centers = np.array([np.mean((emb - real_centers) ** 2, axis=-1)\n",
    "                                    for emb in clusterable_embedding[:, :]])\n",
    "        dist_to_centers = torch.from_numpy(dist_to_centers)\n",
    "        if cluster_time:\n",
    "            centers = real_centers[:, :-1]\n",
    "        else:\n",
    "            centers = real_centers\n",
    "        \n",
    "    else:\n",
    "        # cluster using hdbscan\n",
    "        from hdbscan import HDBSCAN\n",
    "        min_samples = 72\n",
    "        hdbscan_labels = [-1]\n",
    "        while len(np.unique(hdbscan_labels)) < min_clusters and min_samples > 2:\n",
    "            hdbscan_clusterer = HDBSCAN(min_samples=min_samples,  #35, \n",
    "                                        cluster_selection_epsilon=0., \n",
    "                                        min_cluster_size=min(100, len(clusterable_embedding) // 10))\n",
    "            hdbscan_labels = hdbscan_clusterer.fit_predict(clusterable_embedding)\n",
    "            min_samples = min_samples // 2\n",
    "        print(\"Min samples: \", min_samples)\n",
    "        # assign outliers (labels == -1) to the previous non-outlier label \n",
    "        clean_hdbscan_labels = []\n",
    "        last_label = np.array(hdbscan_labels[hdbscan_labels != -1])[0]\n",
    "        for label in hdbscan_labels:\n",
    "            if label == -1:\n",
    "                label = last_label\n",
    "            else:\n",
    "                last_label = label\n",
    "            clean_hdbscan_labels.append(label)\n",
    "        labels = np.array(clean_hdbscan_labels)\n",
    "        num_labels = len(np.unique(labels))\n",
    "\n",
    "        \n",
    "        # assign distances to cluster for each point\n",
    "        from hdbscan_utils import *\n",
    "        data = clusterable_embedding\n",
    "        tree = hdbscan_clusterer.condensed_tree_\n",
    "        exemplar_dict = {c: exemplars(c, tree) for c in tree._select_clusters()}\n",
    "        cluster_ids = tree._select_clusters()\n",
    "        raw_tree = tree._raw_tree\n",
    "        all_possible_clusters = np.arange(data.shape[0], raw_tree['parent'].max() + 1).astype(np.float64)\n",
    "        max_lambda_dict = {c:max_lambda_val(c, raw_tree) for c in all_possible_clusters}\n",
    "\n",
    "        point_dict = {c:set(points_in_cluster(c, raw_tree)) for c in all_possible_clusters}\n",
    "        cluster_distances = np.array([combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n",
    "                                                       max_lambda_dict, point_dict, False) for x in range(len(data))])\n",
    "        dist_to_centers =  1 - torch.from_numpy(cluster_distances)\n",
    "        # find cluster representatives by averagin the points per cluster\n",
    "        real_centers = np.array([data[labels == i].mean(axis=0) for i in range(num_labels)])\n",
    "        if cluster_time:\n",
    "            centers = real_centers[:, :-1]\n",
    "        else:\n",
    "            centers = real_centers\n",
    "\n",
    "    show_2d_umap = 0\n",
    "    if show_2d_umap:\n",
    "        from umap import UMAP\n",
    "        # create 2D UMAP embedding to plot\n",
    "        mapper = UMAP(\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.0,\n",
    "            n_components=2,\n",
    "            random_state=42,\n",
    "            metric=\"cosine\",\n",
    "        ).fit(clustering_feats)\n",
    "        # make plot\n",
    "        import umap.plot\n",
    "        umap.plot.output_notebook()\n",
    "        df = pd.DataFrame({\"step\": list(range(len(labels))),\n",
    "                           \"cluster\": labels,\n",
    "                           })\n",
    "        p = umap.plot.interactive(mapper, \n",
    "                                  labels=df[\"cluster\"], \n",
    "                                  #values = df[\"step\"],\n",
    "                                  hover_data=df, point_size=10)\n",
    "        umap.plot.show(p)\n",
    "    # show clusters over time\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(range(len(labels)), labels, s=1.5)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # show cluster dists over time\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i in range(dist_to_centers.shape[1]):\n",
    "        plt.plot(dist_to_centers[:, i], label=str(i))\n",
    "    l = plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # show heatmap \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    show_df = used_tag_df.T.copy()\n",
    "    show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "    show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# listen to clusters\n",
    "\n",
    "cluster_idx = 0\n",
    "\n",
    "samples_per_step = int(len(song) / len(hdbscan_labels)) + 1\n",
    "frame_assignments = []\n",
    "for label in hdbscan_labels:\n",
    "    frame_assignments.extend([label] * samples_per_step)\n",
    "frame_assignments = np.array(frame_assignments)\n",
    "sections = frame_assignments == cluster_idx\n",
    "song_section = song[sections[:len(song)]]\n",
    "\n",
    "IPython.display.Audio(song_section, rate=sr, autoplay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_stories_and_weights(cluster_gpt_stories, n_start_prompts, dist_to_centers, gpt_story_top_k, idx):\n",
    "    if cluster_gpt_stories is not None:\n",
    "        story_idx = max(idx - n_start_prompts, 0)\n",
    "        top_k = dist_to_centers[story_idx].topk(k=gpt_story_top_k, largest=False)\n",
    "        story_weights = (1 - (top_k.values / dist_to_centers[story_idx].max())) ** 2\n",
    "        top_idcs = top_k.indices\n",
    "        gpt_stories = [cluster_gpt_stories[i] for i in top_idcs]\n",
    "    else:\n",
    "        gpt_stories = [\"\"]\n",
    "        story_weights = [1]\n",
    "    return gpt_stories, story_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_tag_df.mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-final",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest_classification\n",
    "\n",
    "def filter_theme_labels(series, n=6, threshold=0.5, factor=0.9):\n",
    "    # sort by logits\n",
    "    cluster_theme = series.sort_values(ascending=False)\n",
    "    #print(cluster_theme)\n",
    "    # filter logits below certain value\n",
    "    cluster_theme = cluster_theme[cluster_theme > threshold]\n",
    "    # filter logits that are lower than mean over all\n",
    "    cluster_theme = cluster_theme[[col for col in cluster_theme.index if cluster_theme[col] > factor * used_tag_df.mean()[col]]]\n",
    "    # pick top N\n",
    "    cluster_theme = cluster_theme.iloc[:n]\n",
    "    return cluster_theme\n",
    "\n",
    "\n",
    "center_df = pd.DataFrame(centers, columns=used_tag_df.columns)\n",
    "\n",
    "cluster_themes = []\n",
    "for i in range(len(center_df)):\n",
    "    cluster_theme = filter_theme_labels(center_df.iloc[i])\n",
    "    cluster_vals = round(cluster_theme, 2).to_list()\n",
    "    cluster_theme_names = cluster_theme.index.to_list()\n",
    "    cluster_theme_names = \", \".join(cluster_theme_names).lower()\n",
    "    print(str(i) + \":\",  cluster_theme_names, cluster_vals)\n",
    "    cluster_themes.append(cluster_theme_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_theme = used_tag_df.mean().sort_values(ascending=False).iloc[:5]\n",
    "main_theme_words = \", \".join(main_theme.index.to_list())\n",
    "main_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main distinctive features \n",
    "print(\", \".join(center_df.std().sort_values(ascending=False)[:5].index.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# listen to clusters\n",
    "\n",
    "cluster_idx = 0\n",
    "\n",
    "plt.scatter(range(len(labels)), labels, s=1.5)\n",
    "plt.show()\n",
    "\n",
    "samples_per_step = int(len(song) / len(labels)) + 1\n",
    "\n",
    "frame_assignments = []\n",
    "for label in labels:\n",
    "    frame_assignments.extend([label] * samples_per_step)\n",
    "frame_assignments = np.array(frame_assignments)\n",
    "\n",
    "sections = frame_assignments == cluster_idx\n",
    "\n",
    "song_section = song[sections[:len(song)]]\n",
    "\n",
    "IPython.display.Audio(song_section, rate=sr, autoplay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80836b43-1a3f-4e1f-9380-e3631a8425f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1e82e-a241-4fa8-b102-6b652ddada3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# create musicnn prompts\n",
    "clip_prompts = []\n",
    "pbar = tqdm(list(used_tag_df.iterrows()))\n",
    "\n",
    "for i, row in pbar:\n",
    "    row = row[row > tag_df_means]\n",
    "    sorted_row = row.sort_values(ascending=False)\n",
    "\n",
    "    # generate clip prompt for current musicnn targets\n",
    "    if prompt_mode == \"top_k\":\n",
    "        # get tags\n",
    "        top_tag_names = list(sorted_row.iloc[:k].index)\n",
    "        #print(top_tag_names)\n",
    "        pbar.set_description(\", \".join(top_tag_names))\n",
    "        clip_prompt = \", \".join(top_tag_names)\n",
    "    elif prompt_mode == \"weighted_top_k\":\n",
    "        sorted_row = filter_theme_labels(sorted_row, n=k)#, factor=0.8, threshold=0.1)\n",
    "        top_tag_names = list(sorted_row.index)\n",
    "        top_tag_vals = list(sorted_row)\n",
    "        clip_prompt = {name: val for name, val in zip(top_tag_names, top_tag_vals)}\n",
    "    elif prompt_mode == \"gpt\":\n",
    "        sorted_row = row.sort_values(ascending=False)\n",
    "        top_tags = sorted_row.iloc[:k]\n",
    "        top_tag_names = list(top_tags.index)\n",
    "        if len(top_tag_names) == 0:\n",
    "            top_tag_names = [\"Undecided emptiness\"]\n",
    "        merged_top_tags = \", \".join(top_tag_names)\n",
    "        if merged_top_tags in prompt_hash_table:\n",
    "            clip_prompt = prompt_hash_table[merged_top_tags]\n",
    "        else:\n",
    "            clip_prompt = gpt_create_prompt(gpt_model, gpt_tokenizer, merged_top_tags)\n",
    "            pbar.set_description(\"Tags: \" + merged_top_tags + \" Prompt: \" + clip_prompt)\n",
    "            #clip_encoding = imagine.create_text_encoding(clip_prompt)\n",
    "            prompt_hash_table[merged_top_tags] = clip_prompt\n",
    "            \n",
    "    clip_prompts.append(clip_prompt)\n",
    "    \n",
    "# how many steps are there to fill at the start of the song (256 is the size of the fft-windows of musicnn)\n",
    "start_prompt = clip_prompts[0]\n",
    "n_start_prompts = int(np.round((len(song) / (256 * averaging_window) - len(used_tag_df))))\n",
    "clip_prompts.extend([start_prompt] * n_start_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97afad-1e91-4323-885f-81477d8948f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_lyrics:    \n",
    "    def time_str_to_seconds(time_str):\n",
    "        hrs_str = time_str.split(\":\")[0]\n",
    "        min_str = time_str.split(\":\")[1]\n",
    "        seconds_str = time_str.split(\":\")[2].split(\",\")[0]\n",
    "        ms_str = time_str.split(\":\")[2].split(\",\")[1]\n",
    "        return int(hrs_str) * 360 + int(min_str) * 60 + int(seconds_str) + int(ms_str) / 1000\n",
    "\n",
    "    if text_file is None:\n",
    "        lyrics_path = \".\".join(song_name.split(\".\")[:-1]) + \".srt\"\n",
    "\n",
    "        # read file\n",
    "        with open(lyrics_path, \"r+\") as f:\n",
    "            lines = [l for l in f]\n",
    "        # format and extract lines\n",
    "        #print(lines[:10])\n",
    "        texts = []\n",
    "        start_times = []\n",
    "        end_times = []\n",
    "        while len(lines) > 2:\n",
    "            if lines[0] == \"\\n\":\n",
    "                del lines[0]\n",
    "                continue\n",
    "            #print(\"next lines\")\n",
    "            #print(lines[0].strip(\"\\n\"))\n",
    "            #print(lines[1].strip(\"\\n\"))\n",
    "            #print(lines[2].strip(\"\\n\"))\n",
    "            count = 4\n",
    "            # read times\n",
    "            start_time_str = lines[1].split(\" \")[0]\n",
    "            end_time_str = lines[1].split(\" \")[-1]\n",
    "            # convert to seconds\n",
    "            start_time = time_str_to_seconds(start_time_str)\n",
    "            end_time = time_str_to_seconds(end_time_str)\n",
    "\n",
    "            # read text\n",
    "            text = lines[2].strip(\"♪.\\n\")\n",
    "            if len(lines) > 3 and lines[3] != \"\\n\":\n",
    "                text += \" \" + lines[3].strip(\"♪.\\n\")\n",
    "                count += 1\n",
    "                if lines[4] != \"\\n\":\n",
    "                    text += \" \" + lines[4].strip(\"♪.\\n\")\n",
    "                    count += 1\n",
    "                    if lines[5] != \"\\n\":\n",
    "                        text += \" \" + lines[5].strip(\"♪.\\n\")\n",
    "                        count += 1\n",
    "            # remove formatting commands\n",
    "            while text.find(\"<\") != -1:\n",
    "                start = text.find(\"<\")\n",
    "                end = text.find(\">\")\n",
    "                text = text[:start] + text[end + 1:]\n",
    "            # remove spaces and turn to lower-case\n",
    "            text = text.strip(\"♪ .\\n\").lower()\n",
    "\n",
    "            # save\n",
    "            if len(text) > 2:\n",
    "                texts.append(text)\n",
    "                start_times.append(start_time)\n",
    "                end_times.append(end_time)\n",
    "\n",
    "            # delete lines to move on in file\n",
    "            del lines[:count]\n",
    "    else:\n",
    "        with open(text_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        assert len(lines) == 1\n",
    "        texts = [s for s in lines[0].split(\". \") if len(s) > 2]\n",
    "        \n",
    "        num_seconds = len(song) / sr\n",
    "        times =  np.linspace(0, num_seconds, len(texts))#.astype(int)\n",
    "        start_times = times[0:-1]\n",
    "        end_times = times[1:]\n",
    "                \n",
    "    from deep_translator import  GoogleTranslator\n",
    "\n",
    "    #!python3 -m pip install deep-translator\n",
    "\n",
    "    translated_texts = [GoogleTranslator(source='auto', target='en').translate(text=text)\n",
    "                        for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87444723-f447-46df-b5fa-67f08d6b4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate .srt file if read from .txt file\n",
    "if use_lyrics and text_file is not None:\n",
    "    #!python3 -m pip install srt\n",
    "    import srt\n",
    "    from srt import Subtitle\n",
    "    from datetime import timedelta\n",
    "\n",
    "    subtitles = []\n",
    "    for i in range(len(start_times)):\n",
    "        content = texts[i]\n",
    "        start = timedelta(seconds=start_times[i])\n",
    "        end = timedelta(seconds=end_times[i])\n",
    "        subtitle = srt.Subtitle(i, start, end, content)\n",
    "        subtitles.append(subtitle)\n",
    "    subtitle_text = srt.compose(subtitles)\n",
    "    \n",
    "    # write subtitle text to file\n",
    "    file_path = text_file.replace(\".txt\", \"\") + \"_\" + song_name.split(\".\")[0].split(\"/\")[-1] + \".srt\"\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(subtitle_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d14380-6498-4d5e-b493-4c56387dab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match texts with start and end time to frames\n",
    "if use_lyrics:\n",
    "    num_seconds = len(song) / sr\n",
    "    seconds_per_step = num_seconds / len(clip_prompts)\n",
    "\n",
    "    prompt = \"\"\n",
    "    count = 0\n",
    "    lyric_prompts = []\n",
    "    for i in range(len(clip_prompts)):\n",
    "        current_time = i * seconds_per_step\n",
    "        while current_time >= end_times[count]:\n",
    "            prompt = \"\"\n",
    "            count += 1\n",
    "            #if count >= len(clip_prompts):\n",
    "            #    break\n",
    "        if current_time >= start_times[count]:\n",
    "            prompt = translated_texts[count]\n",
    "        lyric_prompts.append(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../StyleCLIP_modular\")\n",
    "sys.path.append(\"../CLIPGuidance\")\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from style_clip import Imagine, create_text_path\n",
    "\n",
    "args = {}\n",
    "args[\"lr_schedule\"] = 0\n",
    "args[\"seed\"] = 1\n",
    "\n",
    "args[\"neg_text\"] = None #'text, signature, watermarks' #'incoherent, confusing, cropped, watermarks'\n",
    "#'text, signature, watermarks, writings, scribblings'#\n",
    "\n",
    "args[\"clip_names\"] = [\"ViT-B/16\", \"ViT-B/32\", \"RN50\"]\n",
    "args[\"averaging_weight\"] = 0\n",
    "args[\"early_stopping_steps\"] = 0\n",
    "args[\"tv_loss_scale\"] = 0.0\n",
    "args[\"lpips_weight\"] = lpips_weight\n",
    "args[\"lpips_batch_size\"] = 4\n",
    "args[\"lpips_net\"] = \"squeeze\"\n",
    "args[\"use_russell_transform\"] = 1\n",
    "\n",
    "if net == \"vqgan\":\n",
    "    args[\"model_type\"] = \"vqgan\"\n",
    "    args[\"lr\"] = 0.03\n",
    "    args[\"batch_size\"] = clip_batch_size\n",
    "elif net == \"conv\":\n",
    "    args[\"act_func\"] = \"gelu\"\n",
    "    args[\"stride\"] = 1\n",
    "    args[\"num_layers\"] = 5\n",
    "    args[\"downsample\"] = False\n",
    "    args[\"norm_type\"] = \"layer\"\n",
    "    args[\"num_channels\"] = 64\n",
    "    args[\"sideX\"] = 1080\n",
    "    args[\"sideY\"] = 720\n",
    "    args[\"lr\"] = 0.005\n",
    "    args[\"stack_size\"] = 4\n",
    "elif net == \"stylegan\":\n",
    "    args[\"style\"] = style\n",
    "    args[\"lr\"] = 0.005\n",
    "    args[\"opt_all_layers\"] = 1\n",
    "elif net == \"image\":\n",
    "    args[\"lr\"] = 0.005\n",
    "    args[\"batch_size\"] = 32\n",
    "    args[\"stack_size\"] = 1\n",
    "elif net == \"dip\":\n",
    "    args[\"lr\"] = 0.00005\n",
    "    args[\"batch_size\"] = 16\n",
    "    args[\"stack_size\"] = 1\n",
    "    args[\"optimizer\"] = \"madgrad\"\n",
    "args[\"model_type\"] = net\n",
    "\n",
    "args[\"sideX\"] = sideX # 688 #624 #544 #480 \n",
    "args[\"sideY\"] = sideY # 384 #352 #304 #272 \n",
    "# 688x384 - 7.792GB, 34s/it\n",
    "# 720x400 - 7.948GB, 41.3s/it - crashes after a bit\n",
    "# 624x352 - 6.9GB, 29.8s/it\n",
    "# 544x304 - 5850MB, 24s/it at 100its per step\n",
    "args[\"circular\"] = 0\n",
    "\n",
    "imagine = Imagine(\n",
    "                save_progress=False,\n",
    "                open_folder=False,\n",
    "                save_video=False,\n",
    "                verbose=False,\n",
    "                use_mixed_precision=True,\n",
    "                **args\n",
    "               )\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import importlib\n",
    "import mustovi_utils\n",
    "import gpt_j_low_prec\n",
    "importlib.reload(mustovi_utils)\n",
    "importlib.reload(gpt_j_low_prec)\n",
    "\n",
    "from mustovi_utils import load_gpt_model, gen_sent\n",
    "from clip import tokenize\n",
    "clip_model = imagine.perceptor.models[0]\n",
    "gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prefix = \"The following are adjectives describing a song, followed by a description of the corresponding image:\\n \"\n",
    "prefix = \"The following are adjectives describing an image, listed in the order of importance. They are followed by a full description of the corresponding image:\\n \"\n",
    "prompter = \". Full description:\"\n",
    "examples = {\"sad, dark, fast\": \" A man is running through dark woods while crying.\",\n",
    "            \"sad, beautiful, soft, quiet, slow\": \" An old woman is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors forming strong geometric shapes that resemble a rave.\",\n",
    "            \"weird, happy, fast\": \" A man is experiencing a strange dream. He is struggling to feel his feelings, his emotions as they rush too quickly through his body. He is an a state of ecstacy.\",\n",
    "            \"harmonious, mellow\": \" An electric light begins to dim at a distant point in the sky. You feel complete and at one with your environment.\",\n",
    "            #\"slow, quiet\": \" A lion's roar stops in front of him. The lion is slowly moving forward and approaching you. He is silent.\"\n",
    "           }\n",
    "\n",
    "target_text = cluster_themes[3]#\"fast, brutal\"\n",
    "target_clip_feats = clip_model.encode_text(tokenize(target_text).to(\"cuda\"))\n",
    "target_text\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"An ancient dream starts to calm down. It is quiet and peaceful\"\n",
    "\"An ordinary man appears before you. He is listening to you. He is a crazed crazy crazy crazy crazy crazy. He is beating on your legs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts, df = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "#             start_text=\" Sweet dance candy\", p=0.94, \n",
    "#            prefix=prefix, examples=examples, prompter=prompter, target_text=target_text,\n",
    "#            clip_weight=0.7, \n",
    "#            clip_temp=0.45, gpt_temp=0.75, out_len=50, v=1, num_beams=10, return_num=5)\n",
    "#print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67a6c7-0f99-438d-a890-579636634873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sort_values(\"post_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21519f74-6e69-449a-83e3-5ae3ddc9cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, row in df.iterrows():\n",
    "#    print(row[1:])#.drop(columns=[\"sent\"]))\n",
    "#    print(row[\"sent\"].strip())\n",
    "#    \n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from mustovi_utils import load_gpt_model, gen_sent\n",
    "from clip import tokenize\n",
    "\n",
    "\n",
    "def gpt_create_prompt(cluster_words_list, gpt_name, clip_model, gpt_model=None, gpt_tokenizer=None, gpt_prefix=\"\"):\n",
    "    if gpt_model is None:\n",
    "        gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)\n",
    "\n",
    "    prefix = \"The following are adjectives describing a song, followed by a description of the corresponding image:\\n \"\n",
    "    prefix = \"The following are adjectives describing an image, listed in the order of importance. They are followed by a full description of the corresponding image:\\n \"\n",
    "    prompter = \". Full description:\"\n",
    "    #examples = {\"sad, dark, fast\": \" A man is running through dark woods while crying.\",\n",
    "    #            \"sad, beautiful, soft, quiet, slow\": \" An old woman is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "    #            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors forming strong geometric shapes that resemble a rave.\",\n",
    "    #           }\n",
    "    examples = {\"sad, dark, fast\": \" Running through dark woods while crying.\",\n",
    "            \"sad, beautiful, soft, quiet, slow\": \" An old widow is sitting on a chair in a beautiful garden with her hands folded in front of her. She is looking at you with a sad expression on her face.\",\n",
    "            \"electronic, loud, happy, abstract\": \" Dynamic and vibrant colors are forming strong geometric shapes that resemble a rave.\",\n",
    "            \"weird, happy, fast\": \" A man is experiencing a strange dream. He is struggling to feel his emotions as they rush too quickly through his body. He is an a state of ecstacy.\",\n",
    "            \"harmonious, mellow\": \" An electric light begins to dim at a distant point in the sky. You feel complete and at one with your environment.\",\n",
    "            \"slow, quiet\": \" A lion's roar stops in front of him. The lion is slowly moving forward and approaching you. He is silent.\"\n",
    "           }\n",
    "\n",
    "    \n",
    "    gpt_stories = []\n",
    "    for target_text in tqdm(cluster_words_list):\n",
    "        target_clip_feats = clip_model.encode_text(tokenize(target_text).to(\"cuda\"))\n",
    "\n",
    "        texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "                 start_text=gpt_prefix, p=0.93, \n",
    "                 prefix=prefix, examples=examples, prompter=prompter, target_text=target_text,\n",
    "                 clip_weight=0.7, \n",
    "                 clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=1)\n",
    "        \n",
    "        text = texts[0]\n",
    "        gpt_stories.append(text)\n",
    "        print(text)\n",
    "        print()\n",
    "    gpt_model = gpt_model.to(\"cpu\")\n",
    "    return gpt_stories\n",
    "\n",
    "used_gpt_stories = None\n",
    "if do_create_gpt_cluster_stories:\n",
    "    used_gpt_stories = gpt_create_prompt(cluster_themes, gpt_name, imagine.perceptor.models[0], gpt_prefix=gpt_cluster_prefix)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_gpt_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mustovi_utils import load_gpt_model, gen_sent\n",
    "from clip import tokenize\n",
    "\n",
    "\n",
    "def gpt_create_theme(theme_words, gpt_name, clip_model, gpt_model=None, gpt_tokenizer=None):\n",
    "    if gpt_model is None:\n",
    "        gpt_model, gpt_tokenizer = load_gpt_model(gpt_name)\n",
    "\n",
    "    prefix = \"The following are adjectives, followed by a matching artstyle:\\n \"\n",
    "    prompter = \". Matching artstyle:\"\n",
    "    \n",
    "    prompter = \"The name of a matching painter is:\"\n",
    "    prefix= \"The following are lists of words describing art, followed by the name of the artist:\\n \"\n",
    "    \n",
    "    prefix = \"The following are lists of words describing art, followed by the name of the artist:\\n \"\n",
    "    prompter = \"Matching visual artist:\"\n",
    "    \n",
    "    prefix = \"The following are lists of adjectives, listed in order of importance. They are followed by a name of an artstyle that matches them:\\n \"\n",
    "    prompter = \". Matching artstyle:\"\n",
    "        \n",
    "    examples = {\"introspective, beautiful, sad\": \" A moody, ambient painting.\",\n",
    "                \"expressive, wild, colourful\": \" An expressionist piece of art.\",\n",
    "                \"epic, fantasy, stunning, moody\": \" Illustrated by Greg Rutkowski.\",\n",
    "                \"introspective, beautiful, sad\": \" Impressionism.\",\n",
    "                \"realistic, beautiful, landscapes, forgotten civilizations\": \" By James Gurney.\"}\n",
    "    # popular, internet{prompter} Trending on artstation.\n",
    "    # rendered, detailed, high-quality{prompter} Rendered in unreal engine.\n",
    "    # expressionist, beautiful, vibrant. {prompter} Van Gogh.\n",
    "    # happy, dreamy, romantic, sensual. {prompter} Gustav Klimt.\n",
    "    \n",
    "    target_clip_feats = clip_model.encode_text(tokenize(theme_words).to(\"cuda\"))\n",
    "\n",
    "    texts = gen_sent(gpt_model, gpt_tokenizer, clip_model, target_clip_feats, \n",
    "                 start_text=\"\", p=0.9, \n",
    "                 prefix=prefix, examples=examples, prompter=prompter, target_text=theme_words,\n",
    "                 clip_weight=0.2, \n",
    "                 clip_temp=0.45, gpt_temp=0.75, out_len=50, v=-1, num_beams=50, return_num=5)    \n",
    "    text = texts[0]\n",
    "    print(texts)\n",
    "    gpt_model = gpt_model.to(\"cpu\")\n",
    "    return text\n",
    "\n",
    "gpt_theme = \"\"\n",
    "if create_gpt_artstyle:\n",
    "    gpt_theme = gpt_create_theme(main_theme_words.lower(), gpt_name, imagine.perceptor.models[0])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gpt_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clip_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100990a-7421-4706-ba7e-b19a45bb09a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate encodings based on prompts\n",
    "\n",
    "clip_target_encodings = []\n",
    "clip_feature_hash_table = dict()\n",
    "gpt_suffix = \"\" if len(gpt_theme) == 0 else f\" {gpt_theme}\"\n",
    "\n",
    "count = []\n",
    "\n",
    "def encode(prompt):\n",
    "    prompt = prefix + prompt\n",
    "    if general_theme is not None:\n",
    "        prompt = prompt + general_theme\n",
    "    prompt += gpt_suffix\n",
    "    if prompt in clip_feature_hash_table:\n",
    "        encoding = clip_feature_hash_table[prompt]\n",
    "    else:\n",
    "        count.append(0)\n",
    "        if len(count) % 50 == 0:\n",
    "            print(prompt)\n",
    "        encoding = imagine.create_clip_encoding(text=prompt, img=img_theme)\n",
    "        #encoding = imagine.create_text_encoding(prompt)\n",
    "        clip_feature_hash_table[prompt] = encoding\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def weighted_average_encoding(encodings, weights):\n",
    "    clip_encoding = [norm(torch.stack([norm(encoding[j]) * weight for encoding, weight in zip(encodings, weights)]).sum(dim=0))\n",
    "                         for j in range(len(encodings[0]))]\n",
    "    return clip_encoding\n",
    "\n",
    "\n",
    "def norm(a):\n",
    "    return a / a.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def clip_mean_direction(direction_prompt, base_prompts, imagine):\n",
    "    base_encs = [imagine.create_clip_encoding(text=p) for p in base_prompts]\n",
    "    base_plus_dir_encs = [imagine.create_clip_encoding(text=p + direction_prompt) for p in base_prompts]\n",
    "    diff_encs = [[norm(norm(base_ext_enc[i]) - norm(base_enc[i])) for i in range(len(base_enc))] for base_enc, base_ext_enc in zip(base_encs, base_plus_dir_encs)]\n",
    "    mean_diff_enc = [norm(torch.stack([diff_encs[j][i] for j in range(len(diff_encs))]).mean(dim=0)) for i in range(len(diff_encs[0]))]\n",
    "    return mean_diff_enc\n",
    "\n",
    "\n",
    "\n",
    "if use_mean_dirs:\n",
    "    base_prompts = [\"A photo of \", \" \", \"A painting of \", \"This painting is: \", \"This photo looks \", \"I feel \", \"I feel: \", \"The sky is \",\n",
    "                   \"This is \", \"The ground is \", \"This person is \", \"She is \", \"He  is \", \"A \"]\n",
    "    dir_dict = {col: clip_mean_direction(col, base_prompts, imagine) for col in used_tag_df}\n",
    "\n",
    "\n",
    "for idx, prompt in enumerate(tqdm(clip_prompts)):\n",
    "    gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                             dist_to_centers, gpt_story_top_k, idx)\n",
    "    \n",
    "    if use_lyrics:\n",
    "        lyrics_prompt = lyric_prompts[idx] + \". \"\n",
    "    else:\n",
    "        lyrics_prompt = \"\"\n",
    "    \n",
    "    story_encodings = []\n",
    "    for gpt_story, story_weight in zip(gpt_stories, story_weights):\n",
    "        if isinstance(prompt, dict):\n",
    "            if use_mean_dirs:\n",
    "                take_avg = True\n",
    "                \n",
    "                base_encoding = encode(lyrics_prompt + gpt_story + \" \")\n",
    "                \n",
    "                if take_avg:\n",
    "                    dir_encodings = [dir_dict[tag] for tag in prompt]\n",
    "                    weights = list(prompt.values())\n",
    "                    if len(dir_encodings) > 0:                        \n",
    "                        dir_mean_encoding = weighted_average_encoding(dir_encodings, weights)\n",
    "                        clip_encoding = weighted_average_encoding([base_encoding, dir_mean_encoding], [1 - mood_weight, mood_weight])\n",
    "                else:\n",
    "                    for tag in prompt:\n",
    "                        weight = prompt[tag]\n",
    "                        clip_encoding = [base_encoding[i] + dir_dict[tag][i] * weight for i in range(len(base_encoding))]\n",
    "                clip_encoding = [norm(enc) for enc in clip_encoding]\n",
    "            else:\n",
    "                if len(prompt) > 0:\n",
    "                    encs_suff = [encode(lyrics_prompt + gpt_story + \" It feels \" + prompt_key + \".\")\n",
    "                                        for prompt_key in prompt]\n",
    "                    encs_pre = [encode(\"It feels \" + prompt_key + \". \" + lyrics_prompt + gpt_story) \n",
    "                                        for prompt_key in prompt]\n",
    "                    encodings = [weighted_average_encoding([encs_suff[i], encs_pre[i]], [1.0, 1.0])\n",
    "                                for i in range(len(encs_suff))]\n",
    "                    weights = list(prompt.values())\n",
    "                    mood_weighted_mean_encoding = weighted_average_encoding(encodings, weights)\n",
    "                    base_encoding = encode(lyrics_prompt + gpt_story + \" \")\n",
    "                    clip_encoding = weighted_average_encoding([base_encoding, mood_weighted_mean_encoding],\n",
    "                                                              [1 - mood_weight, mood_weight])\n",
    "                else:\n",
    "                    clip_encoding = encode(lyrics_prompt + gpt_story + \" \")\n",
    "\n",
    "        else:\n",
    "            story_prompt = lyrics_prompt + gpt_story + \" \" + prompt + \".\"\n",
    "            clip_encoding = encode(story_prompt)\n",
    "        story_encodings.append(clip_encoding)\n",
    "    clip_encoding = weighted_average_encoding(story_encodings, story_weights)\n",
    "\n",
    "    \n",
    "    clip_encoding = [enc.to(\"cpu\") for enc in clip_encoding]\n",
    "    clip_target_encodings.append(clip_encoding)\n",
    "    \n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test directions\n",
    "\"\"\"\n",
    "print(dir_dict.keys())\n",
    "base_text = \"Home.\"\n",
    "steps = 150\n",
    "text_weight = 0.4\n",
    "dirs = [\"slow\", \"experimental\"]\n",
    "weights = [1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "text_enc = imagine.create_text_encoding(base_text)\n",
    "\n",
    "\n",
    "dir_encodings = [dir_dict[tag] for tag in dirs]\n",
    "print(dir_encodings[0][0][0][:10])\n",
    "dir_mean_encoding = weighted_average_encoding(dir_encodings, weights)\n",
    "clip_encoding = weighted_average_encoding([text_enc, dir_mean_encoding], [text_weight, 1 - text_weight])\n",
    "print(clip_encoding[0][0][:10])\n",
    "\n",
    "#for p, w in zip(dirs, weights):\n",
    "#    dir_enc = dir_dict[p]\n",
    "#    clip_encoding = [norm(text_enc[i] * text_weight + dir_enc[i] * (1 - text_weight)) for i in range(len(dir_enc))]\n",
    "\n",
    "    \n",
    "imagine.set_clip_encoding(encoding=clip_encoding)\n",
    "imagine.reset()\n",
    "for _ in tqdm(range(steps)):\n",
    "    img, loss = imagine.train_step(0, 0)\n",
    "\"\"\"\n",
    "# to_pil(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip_target_encodings[10][0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ema of encodings to smoothen\n",
    "ema_encodings = []\n",
    "ema = clip_target_encodings[0]\n",
    "\n",
    "for encoding in clip_target_encodings:\n",
    "    ema = [ema_val * ema[i].to(\"cpu\") + (1 - ema_val) * encoding[i].to(\"cpu\") for i in range(len(encoding))]\n",
    "    ema_encodings.append(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "from mustovi_utils import get_spec_norm\n",
    "import librosa\n",
    "\n",
    "# create zoom, rotate, shift effects\n",
    "effects = [\"zoom\", \"rotate\", \"shiftX\", \"shiftY\", \"shear\"]\n",
    "harm_effect_dict =dict()  #{\"rotate\": 0.0}\n",
    "perc_effect_dict = dict() #{\"zoom\": -0.5}\n",
    "old_cqt_effect_dict = [{\"zoom\": 1.0}, \n",
    "                   {\"rotate\": 1.0},\n",
    "                   {\"shiftX\": 1.0}, \n",
    "                   {\"shiftY\": 1.0},\n",
    "                   {\"shiftY\": -1.0},\n",
    "                   {\"shiftX\": -1.0},\n",
    "                   {\"rotate\": -1.0},\n",
    "                   {\"zoom\": -1.0},\n",
    "                  ]\n",
    "cqt_effect_dict = [{\"zoom\": -1.00}, \n",
    "                   {\"zoom\": -0.5},\n",
    "                   {\"rotate\": 0.75}, \n",
    "                   {\"rotate\": 0.5},\n",
    "                   {\"rotate\": -0.5},\n",
    "                   {\"rotate\": -0.75},\n",
    "                   {\"zoom\": 0.5},\n",
    "                   {\"zoom\": 1.0},\n",
    "                  ]\n",
    "                    \n",
    "# divide song in percussion and harm (might divide in pitches later)\n",
    "song_harm, song_perc = librosa.effects.hpss(song)\n",
    "spec_norm_harm = get_spec_norm(song_harm)\n",
    "spec_norm_perc = get_spec_norm(song_perc)\n",
    "# get cqt spec\n",
    "n_chroma = len(cqt_effect_dict)\n",
    "cqt_spec = librosa.feature.chroma_cqt(y=song, sr=sr,hop_length=256, \n",
    "                                      n_chroma=n_chroma, n_octaves=7, \n",
    "                                      bins_per_octave=n_chroma * 4, norm=None)\n",
    "sns.heatmap(cqt_spec)\n",
    "plt.show()\n",
    "# take window averages to match video fps\n",
    "N = averaging_window\n",
    "spec_norm_harm = np.convolve(spec_norm_harm, np.ones(N) / N , mode='valid')[::N]\n",
    "spec_norm_perc = np.convolve(spec_norm_perc, np.ones(N) /N, mode='valid')[::N]\n",
    "cqt_spec = np.array([np.convolve(cqt_line, np.ones(N) / N, mode='valid')[::N] \n",
    "                     for cqt_line in cqt_spec])\n",
    "# min-max norm\n",
    "spec_norm_harm = (spec_norm_harm - spec_norm_harm.min()) / (spec_norm_harm.max() - spec_norm_harm.min())\n",
    "spec_norm_perc = (spec_norm_perc - spec_norm_perc.min()) / (spec_norm_perc.max() - spec_norm_perc.min())\n",
    "cqt_spec = (cqt_spec - cqt_spec.min()) / (cqt_spec.max() - cqt_spec.min())\n",
    "# create effects\n",
    "import kornia\n",
    "    \n",
    "class Effect:\n",
    "    def __init__(self, strength, zoom=0, rotate=0, \n",
    "                 shiftX=0, shiftY=0, shear=0):\n",
    "        max_zoom = 0.22\n",
    "        self.zoom = 1 + max_zoom * zoom * strength\n",
    "        max_rotate = 10\n",
    "        self.rotate = max_rotate * rotate * strength\n",
    "        max_shift = 12\n",
    "        self.shift_x = max_shift * shiftX * strength\n",
    "        self.shift_y = max_shift * shiftY * strength\n",
    "        \n",
    "        self.shear_x = 0\n",
    "        self.shear_y = 0\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # transform it\n",
    "        matrix = kornia.geometry.transform.get_affine_matrix2d(translations=torch.tensor([(self.shift_x, self.shift_y)]).float(), \n",
    "                                                      center=torch.tensor([(img.shape[-1] // 2, img.shape[-2] // 2)]).float(), \n",
    "                                                      scale=torch.tensor([(self.zoom, self.zoom)]).float(), \n",
    "                                                      angle=torch.tensor([self.rotate]).float(), \n",
    "                                                      sx=torch.tensor([self.shear_x]).float(),\n",
    "                                                      sy=torch.tensor([self.shear_y]).float())\n",
    "        if img.ndim < 4:\n",
    "            img = img.unsqueeze(0)\n",
    "        transformed = kornia.geometry.transform.warp_perspective(img, matrix.to(img.dtype).to(img.device), img.shape[-2:],\n",
    "                                      mode='bilinear', padding_mode='reflection', align_corners=True)\n",
    "        return transformed\n",
    "        \n",
    "\n",
    "def merge_dicts(effect_dict, effect_strength_dict, amplitude):\n",
    "    for key in effect_strength_dict:\n",
    "        content = effect_strength_dict[key] * amplitude\n",
    "        if key in effect_dict:\n",
    "            effect_dict[key] += content\n",
    "        else:\n",
    "            effect_dict[key] = content\n",
    "\n",
    "# create effects that directly alter the image\n",
    "if args[\"model_type\"] in (\"vqgan\", \"image\", \"dip\"):\n",
    "    effects_list = []\n",
    "    for i in range(len(spec_norm_harm)):\n",
    "        harm = spec_norm_harm[i]\n",
    "        perc = spec_norm_perc[i]\n",
    "        cqt = cqt_spec[:, i]\n",
    "\n",
    "        effect_dict = {}\n",
    "        merge_dicts(effect_dict, harm_effect_dict, harm)\n",
    "        merge_dicts(effect_dict, perc_effect_dict, perc)\n",
    "        for cqt_effect, cqt_amplitude in zip(cqt_effect_dict, cqt):\n",
    "            merge_dicts(effect_dict, cqt_effect, cqt_amplitude)\n",
    "\n",
    "        effect = Effect(total_effect_strength, **effect_dict)\n",
    "        effects_list.append([effect])\n",
    "else:\n",
    "    # create effects that alter the clip target shortly\n",
    "    # these effects should have a different name\n",
    "    effects_list = [[]] * len(spec_norm_harm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ema_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(effects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(a):\n",
    "    min_ = a.min()\n",
    "    return (a - min_) / (a.max() - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-shore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "substeps_per_step = [sub_steps] * len(ema_encodings)\n",
    "if use_variable_substeps:\n",
    "    # vary number of substeps to take depending on amplitude\n",
    "    use_spec_norm = 0\n",
    "    if use_spec_norm:\n",
    "        spec_norm_df = pd.DataFrame(get_spec_norm(song))\n",
    "        # take step average taggram\n",
    "        fps_spec_norm = spec_norm_df.rolling(averaging_window, min_periods=1, axis=0).mean() \n",
    "        fps_spec_norm = spec_norm_df.iloc[::averaging_window, :]\n",
    "        spec_norm = fps_spec_norm.to_numpy()[:, 0]\n",
    "        spec_norm = minmax(np.array(apply_ema(spec_norm, 0.99)))\n",
    "        substeps_per_step = spec_norm * (max_substeps - min_substeps) + min_substeps \n",
    "    else:\n",
    "        last = used_tag_df.iloc[0]\n",
    "        mood_changes = []\n",
    "        for i, row in used_tag_df.iterrows():\n",
    "            mood_change = np.mean((row - last) ** 2)\n",
    "            mood_changes.append(mood_change)\n",
    "            last = row\n",
    "        plt.plot(np.concatenate([[min_substeps] * n_start_prompts, minmax(np.array(mood_changes)) * (max_substeps - min_substeps) + min_substeps]))\n",
    "        spec_norm = minmax(np.array(apply_ema(mood_changes, 0.95)))\n",
    "        substeps_per_step = spec_norm * (max_substeps - min_substeps) + min_substeps \n",
    "        # add steps for first N steps\n",
    "        substeps_per_step = np.concatenate([[substeps_per_step[0]] * n_start_prompts, substeps_per_step])\n",
    "    plt.plot(substeps_per_step)\n",
    "    substeps_per_step = np.round(substeps_per_step).astype(int)\n",
    "    print(substeps_per_step.mean())\n",
    "    #plt.plot(substeps_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def save_latent(latent, path, net):\n",
    "    if net == \"image\":\n",
    "        latent = torch.clamp(latent, 0, 1)\n",
    "        pil_img = torchvision.transforms.ToPILImage()(latent.squeeze())\n",
    "        jpg_path = path.replace(\".pt\", \".jpg\")\n",
    "        #pil_img.save(jpg_path, quality=95, subsample=0)\n",
    "        \n",
    "        p = multiprocessing.Process(target=pil_img.save, args=(jpg_path,), kwargs={\"quality\": 95, \"subsample\": 0})\n",
    "        p.start()\n",
    "        #p.join\n",
    "    else:\n",
    "        torch.save(latent, path)\n",
    "        \n",
    "        \n",
    "# TODO: use subprocess to save stuff! (maybe also to load?)\n",
    "#     p = multiprocessing.Process(target=save, args=(img, \"test2.jpg\"))\n",
    "    # p.start()\n",
    "#p.join()\n",
    "\n",
    "def load_latent(path, net):\n",
    "    if net == \"image\":\n",
    "        jpg_path = path.replace(\".pt\", \".jpg\")\n",
    "        img = Image.open(jpg_path)\n",
    "        latent = torchvision.transforms.ToTensor()(img)\n",
    "    else:\n",
    "        latent = torch.load(path)\n",
    "    return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7f732-c3fd-4470-a8ff-93e4af11f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5551d-6156-4d0a-8ec9-d1777021e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.gmtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "if len(ema_encodings) > len(effects_list):\n",
    "    ema_encodings = ema_encodings[:-1]\n",
    "if len(ema_encodings) < len(effects_list):\n",
    "    effects_list = effects_list[:-1]\n",
    "assert len(ema_encodings) == len(effects_list), f\"{len(ema_encodings)}, {len(effects_list)}\"\n",
    "\n",
    "\n",
    "img_latents = []\n",
    "time_str = time.strftime(\"%Y_%m_%d__%H_%M_%S\", time.gmtime())\n",
    "save_folder = base_folder + f\"tmp/vid_latents{time_str}/\"\n",
    "if os.path.exists(save_folder):\n",
    "    shutil.rmtree(save_folder)\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "imagine.to(\"cuda\")\n",
    "imagine.reset()\n",
    "imagine.set_clip_encoding(encoding=[item.to(imagine.device) for item in ema_encodings[0]])\n",
    "img, loss = imagine.train_step(0, 0)\n",
    "img = img.detach().cpu()\n",
    "\n",
    "imgs = []\n",
    "save_image = (net == \"dip\")\n",
    "\n",
    "pbar = tqdm(list(range(len(ema_encodings))))\n",
    "for i in pbar:\n",
    "    clip_encoding, effects = ema_encodings[i], effects_list[i]\n",
    "    # apply effects\n",
    "    transformed_img = img.float() \n",
    "    if img is not None and len(effects) > 0:\n",
    "        if net == \"dip\":\n",
    "            effect_img = imagine.model.model.latents.squeeze(0).cpu() + 1\n",
    "            effect_img = effect_img.float().clip(0, 1)\n",
    "        else:\n",
    "            effect_img = transformed_img.cpu()\n",
    "        \n",
    "        for effect in effects:\n",
    "            effect_img = effect(effect_img)\n",
    "\n",
    "        if net == \"dip\":\n",
    "            imagine.model.model.latents = effect_img.unsqueeze(0).to(imagine.device) - 1\n",
    "        else:\n",
    "            if net == \"vqgan\":\n",
    "                effect_img_normed = effect_img.mul(2).sub(1).to(imagine.device)\n",
    "                latent, _, [_, _, indices] = imagine.model.model.model.encode(effect_img_normed)\n",
    "            else:\n",
    "                latent = effect_img\n",
    "            imagine.set_latent(latent)\n",
    "    # set target encoding in CLIP\n",
    "    clip_encoding = [part.to(imagine.device) for part in clip_encoding]\n",
    "    imagine.set_clip_encoding(encoding=clip_encoding)\n",
    "    # optimize for some steps\n",
    "    for _ in range(substeps_per_step[i]):\n",
    "        img, loss = imagine.train_step(0, 0, lpips_img=transformed_img.to(imagine.device))\n",
    "    img = img.detach().cpu()\n",
    "    \n",
    "    # get latent of img\n",
    "    latent = imagine.model.model.get_latent()\n",
    "    if net != \"dip\":\n",
    "        latent = latent.detach().cpu()\n",
    "    if save_folder is None:\n",
    "        img_latents.append(latent)\n",
    "    else:\n",
    "        latent_name = str(i) + \".pt\"\n",
    "        save_latent(latent, save_folder + latent_name, net)\n",
    "    # save final img\n",
    "    if save_image:\n",
    "        imgs.append(to_pil(img.squeeze(0)))\n",
    "    if i % (len(ema_encodings) // 20) == 0:\n",
    "        pil_img = to_pil(img.squeeze())\n",
    "        display(pil_img)\n",
    "\n",
    "#img_latents = sequential_gen(ema_encodings, effects_list)\n",
    "#img_latents = parallel_gen(clip_prompts)\n",
    "\n",
    "# TODO: instead of saving the latent in every step, fill up buffer and save in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c18f7f-2e1a-44c6-baf0-da147798953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_latents(a, b, w_a=1, w_b=1):\n",
    "    if net == \"dip\":\n",
    "        #a = a[1]\n",
    "        #b = b[1]\n",
    "        return [a[1][key] * w_a + b[1][key] * w_b for key in a[1]]\n",
    "    else:\n",
    "        return a * w_a + b * w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ema_disk(in_folder, out_folder, ema_val=0.9):\n",
    "    if os.path.exists(out_folder):\n",
    "        shutil.rmtree(out_folder)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    \n",
    "    num_items = len([item for item in os.listdir(in_folder) if item.endswith(\".pt\") or item.endswith(\".jpg\")])\n",
    "    ema = load_latent(in_folder + \"0.pt\", net)\n",
    "    out = []\n",
    "    for item_idx in tqdm(range(num_items)):\n",
    "        item_name = str(item_idx) + \".pt\"\n",
    "        # load\n",
    "        item = load_latent(in_folder + item_name, net)\n",
    "        # calc\n",
    "        ema = merge_latents(ema, item, ema_val, 1 - ema_val)\n",
    "        #ema = ema * ema_val + item * (1 - ema_val)\n",
    "        # store\n",
    "        save_latent(ema, out_folder + item_name, net)\n",
    "        \n",
    "    if os.path.exists(in_folder):\n",
    "        shutil.rmtree(in_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "if net != \"dip\":\n",
    "    # take ema of encodings to smoothen\n",
    "    in_folder = save_folder\n",
    "    ema_folder = base_folder + f\"tmp/vid_ema_latents_{time_str}/\"\n",
    "    ema_latents = apply_ema_disk(in_folder, ema_folder, ema_val=ema_val_latent)\n",
    "    #ema_latents = apply_ema(img_latents, ema_val=ema_val_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate between latents to increase fps and make video smoother\n",
    "from mustovi_utils import slerp\n",
    "\n",
    "def boost_frames(ema_latents, boost_fps, song):\n",
    "    goal_frame_count = boost_fps * len(song) / 16000\n",
    "    current_frame_count = len(ema_latents)\n",
    "    frames_to_add = np.ceil(goal_frame_count / current_frame_count)\n",
    "    if frames_to_add > 1:\n",
    "        video_latents = []\n",
    "        for i, latent in enumerate(ema_latents):\n",
    "            if i + 1 == len(ema_latents):\n",
    "                next_latent = ema_latents[i + 1]\n",
    "            else:\n",
    "                next_latent = ema_latents[i + 1]\n",
    "            if net == \"dip\":\n",
    "                latents_to_add = [[slerp(latent[i], next_latent[i], frac) for i in range(len(latent))]\n",
    "                                  for frac in np.arange(frames_to_add) / frames_to_add]\n",
    "            else:\n",
    "                latents_to_add = [slerp(latent, next_latent, frac) \n",
    "                                  for frac in np.arange(frames_to_add) / frames_to_add]\n",
    "            video_latents.extend(latents_to_add)\n",
    "        return video_latents\n",
    "    else:\n",
    "        return ema_latents\n",
    "    \n",
    "def boost_frames_disk(in_folder, out_folder, boost_fps, song):\n",
    "    if os.path.exists(out_folder):\n",
    "        shutil.rmtree(out_folder)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    \n",
    "    num_items = len([item for item in os.listdir(in_folder) if item.endswith(\".pt\") or item.endswith(\".jpg\")])\n",
    "    \n",
    "    goal_frame_count = boost_fps * len(song) / 16000\n",
    "    current_frame_count = num_items\n",
    "    frames_to_add = np.ceil(goal_frame_count / current_frame_count)\n",
    "    if frames_to_add > 1:\n",
    "        video_latents = []\n",
    "        count = 0\n",
    "        for i in tqdm(range(num_items)):\n",
    "            item_name = str(i) + \".pt\"\n",
    "            \n",
    "            latent = load_latent(in_folder + item_name, net)\n",
    "            if i + 1 == num_items:\n",
    "                next_latent = latent#ema_latents[i]\n",
    "            else:\n",
    "                next_latent_name = str(i + 1) + \".pt\"\n",
    "                next_latent = load_latent(in_folder + next_latent_name, net)\n",
    "            latents_to_add = [slerp(latent, next_latent, frac) \n",
    "                              for frac in np.arange(frames_to_add) / frames_to_add]\n",
    "            for new_latent in latents_to_add:\n",
    "                item_name = str(count) + \".pt\"\n",
    "                save_latent(new_latent, out_folder + item_name, net)\n",
    "                count += 1\n",
    "    else:\n",
    "        # rename in folder to out folder\n",
    "        os.rename(input_folder, output_folder)\n",
    "        \n",
    "    if os.path.exists(in_folder):\n",
    "        shutil.rmtree(in_folder)\n",
    "    \n",
    "if net != \"dip\":\n",
    "    #video_latents = boost_fps(ema_latents, boost_fps, song)\n",
    "    in_folder = ema_folder\n",
    "    boost_folder = base_folder + f\"tmp/vid_boosted_latents_{time_str}/\"\n",
    "    boost_frames_disk(in_folder, boost_folder, boost_fps, song)\n",
    "    in_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a8673-720a-4d97-bac1-44c7ec756f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_img_paths(root):\n",
    "    paths = [os.path.join(root, f) for f in os.listdir(root)\n",
    "        if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "    paths = sorted(paths, key= lambda x: int(x.split(\"/\")[-1].split(\"_\")[0].split(\".\")[0]))\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.to(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create images and save to disk:\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "if net != \"dip\":\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    extension = \".jpg\"\n",
    "    tmp_folder = base_folder + f\"tmp/vid_imgs_{time_str}/\" \n",
    "    final_latents_folder = boost_folder\n",
    "    #vid_boosted_latents\n",
    "\n",
    "    if os.path.exists(tmp_folder):\n",
    "        shutil.rmtree(tmp_folder)\n",
    "    os.makedirs(tmp_folder, exist_ok=True)\n",
    "\n",
    "    gen_model = imagine.model.model\n",
    "    imagine.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    gen_model.to(device)\n",
    "\n",
    "\n",
    "    num_items = len([item for item in os.listdir(final_latents_folder) if item.endswith(\".pt\") or item.endswith(\".jpg\")])\n",
    "    for i in tqdm(range(num_items)):\n",
    "        latent = load_latent(final_latents_folder + str(i) + \".pt\", net)\n",
    "        if net == \"image\":\n",
    "            img = torch.clamp(latent, 0, 1)\n",
    "            #print(img.min(), img.max())\n",
    "            #img = minmax(img)\n",
    "        else:\n",
    "            img = gen_model(latents=latent.to(device)).to(\"cpu\")\n",
    "        pil_img = to_pil(img.squeeze())\n",
    "        #print(np.array(pil_img).min(), np.array(pil_img).max())\n",
    "        pil_img.save(os.path.join(tmp_folder, f\"{i}{extension}\"), subsampling=0, quality=95)\n",
    "        \n",
    "    if os.path.exists(final_latents_folder):\n",
    "        shutil.rmtree(final_latents_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from PIL import Image\n",
    "import soundfile\n",
    "import moviepy.editor as mpy\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_video(imgs, song, song_name, vid_name, sr, duration, bitrate=\"5000k\"):\n",
    "    os.makedirs(\"video_gens\", exist_ok=True)\n",
    "    video_path = f\"video_gens/{vid_name}\"\n",
    "    \n",
    "    if isinstance(imgs, list):\n",
    "        img_len = len(imgs)\n",
    "    else:\n",
    "        img_len = len([img for img in os.listdir(imgs) \n",
    "                       if img.endswith(\".jpg\") or img.endswith(\".png\")])\n",
    "    \n",
    "    # Generate final video\n",
    "    vid_fps = len(imgs) / (len(song) / sr) #audio.duration\n",
    "    print(vid_fps)\n",
    "    video = mpy.ImageSequenceClip(imgs, fps=vid_fps)\n",
    "    temp_vid_path = \"tmp/video.mp4\"\n",
    "    video.write_videofile(temp_vid_path, \n",
    "                      codec=\"libx264\",\n",
    "                      fps=vid_fps,\n",
    "                      #audio_codec=\"aac\",\n",
    "                      threads=10,\n",
    "                      bitrate=bitrate,\n",
    "                      #audio_bitrate=\"320k\",\n",
    "                      preset=\"slow\",\n",
    "                     )\n",
    "    \n",
    "    command = f\"ffmpeg -i '{temp_vid_path}' -i '{song_name}' -c:v copy -map 0:v:0 -map \\\n",
    "1:a:0 -c:a aac -b:a 512k '{video_path}'\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%m_%d_%H:%M\")  #(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "video_name = f\"{song_name.split('/')[-1].split('.')[0]}_{prompt_mode}_ema{ema_val}_steps{sub_steps}\"\n",
    "video_name += \"_\"+ gpt_theme.replace(\" \", \"_\") if create_gpt_artstyle else \"\"\n",
    "video_name += f\"_{args['model_type']}_{date_time}.mp4\"\n",
    "#video_name = \"loud_pipes_kiefer.mp4\"\n",
    "\n",
    "if net == \"dip\":\n",
    "    paths = [np.array(img) for img in imgs]\n",
    "else:\n",
    "    paths = load_img_paths(tmp_folder)\n",
    "create_video(paths, raw_song, song_name, video_name, old_sr, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
    "if not os.path.exists(\"Real-ESRGAN\"):\n",
    "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "    %cd Real-ESRGAN\n",
    "    # Set up the environment\n",
    "    !$PYTHONPATH -m pip install basicsr facexlib gfpgan\n",
    "    !$PYTHONPATH -m pip install -r requirements.txt\n",
    "    !$PYTHONPATH setup.py develop\n",
    "    # Download the pre-trained model\n",
    "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
    "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth -P experiments/pretrained_models\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine = imagine.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Real-ESRGAN\")\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs(imgs, out_folder=None, scale=4, tile=0):\n",
    "    \n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    upsampler = RealESRGANer(\n",
    "        scale=scale,\n",
    "        model_path=\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\",\n",
    "        model=model,\n",
    "        tile=tile,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=1)\n",
    "    \n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            img = np.array(Image.open(img))[:,:,::-1]\n",
    "        \n",
    "        output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs\n",
    "\n",
    "\n",
    "import torchvision\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs_custom(imgs, out_folder=None, scale=4, tile=0):\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    loadnet = torch.load(\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\")\n",
    "    if 'params_ema' in loadnet:\n",
    "        keyname = 'params_ema'\n",
    "    else:\n",
    "        keyname = 'params'\n",
    "    model.load_state_dict(loadnet[keyname], strict=True)\n",
    "    model.eval().cuda().half()\n",
    "\n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            #img = torch.from_numpy(np.ascontiguousarray(Image.open(img))[:,:,::-1].copy()).unsqueeze(0)\n",
    "            img = torch.from_numpy(np.transpose(np.array(Image.open(img))[:,:,::-1].copy(), (2, 0, 1))).float().unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        output = model(img.half()).cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        #output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        output = (output * 255.0).round().astype(np.uint8)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def load_images_from_mp4(path):\n",
    "    vid = imageio.get_reader(path,  'ffmpeg')\n",
    "    imgs = [np.array(image) for image in vid.iter_data()]\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if upscale:\n",
    "    input_folder = tmp_folder\n",
    "    output_folder = base_folder + f\"tmp/upscaled_vid_imgs_{time_str}\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"Upscaling...\")\n",
    "    \n",
    "    #video_path = \"video_gens/any_colour_you_like_pink_floyd_hd_studio_quality_7032261705832661515_top_k_ema0.2_steps100Weird_and_beautiful._vqgan_11_20_14:02.mp4\"\n",
    "    #video_name = video_path.split(\"/\")[0]\n",
    "    #input_paths = load_images_from_mp4(video_name)\n",
    "    \n",
    "    input_paths = load_img_paths(input_folder)\n",
    "    upscale_imgs(input_paths, out_folder=output_folder, scale=4)\n",
    "    #!CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth --input $input_folder --output $output_folder  --netscale 4 --outscale 4 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    upscaled_video_name = video_name.split(\"/\")\n",
    "    upscaled_video_name[-1] = \"HD_\" + upscaled_video_name[-1]\n",
    "    upscaled_video_name = \"/\".join(upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)    \n",
    "    create_video(paths, raw_song, song_name, upscaled_video_name, old_sr, duration, bitrate=\"12000k\")\n",
    "    if os.path.exists(input_folder):\n",
    "        shutil.rmtree(input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "if twice_upscale:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    input_folder = base_folder + \"tmp/upscaled_vid_imgs\"\n",
    "    output_folder = base_folder + \"tmp/twice_upscaled_vid_imgs\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)    \n",
    "    print(\"Upscaling...\")\n",
    "    input_paths = load_img_paths(input_folder)\n",
    "    upscale_imgs(input_paths, out_folder=output_folder, scale=4)\n",
    "    #!CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x2plus.pth --input  $input_folder --output $output_folder  --netscale 2 --outscale 2 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    twice_upscaled_video_name = upscaled_video_name.split(\"/\")\n",
    "    twice_upscaled_video_name[-1] = \"Full\" + twice_upscaled_video_name[-1]\n",
    "    twice_upscaled_video_name = \"/\".join(twice_upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)\n",
    "    create_video(paths, song_name, twice_upscaled_video_name, old_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-amino",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
